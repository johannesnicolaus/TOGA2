"""
TOGA2 main class
"""

import logging
import os
import subprocess
import sys
import time
from collections import defaultdict
from contextlib import nullcontext
from pathlib import Path
from shutil import copy, which
from typing import Any, Dict, List, Optional, Union

import click

from .constants import TOGA2_SLOT2ARG, TOGA2_SLOTS, Constants
from .parallel_jobs_manager import (
    CustomStrategy,
    NextflowStrategy,
    ParallelJobsManager,
    ParaStrategy,
)
from .results_checks import ResultChecker, SanityCheckResult
from .shared import CommandLineManager, dir_name_by_date, get_upper_dir, hex_dir_name

__author__ = "Yury V. Malovichko"
__version__ = "2.0.6"
__year__ = "2024"
__credits__ = ("Bogdan M. Kirilenko", "Michael Hiller")

LOCATION: str = get_upper_dir(__file__, 4)
PYTHON_DIR: str = get_upper_dir(__file__, 2)
BIN: str = os.path.join(LOCATION, "bin")

sys.path.append(PYTHON_DIR)


class TogaMain(CommandLineManager):
    """
    \b
    MMP""MM""YMM   .g8""8q.     .g8\"""bgd      db          `7MMF'`7MMF'
    P'   MM   `7 .dP'    `YM. .dP'     `M     ;MM:           MM    MM
         MM     dM'      `MM dM'       `     ,V^MM.          MM    MM
         MM     MM        MM MM             ,M  `MM          MM    MM
         MM     MM.      ,MP MM.    `7MMF'  AbmmmqMA         MM    MM
         MM     `Mb.    ,dP' `Mb.     MM   A'     VML        MM    MM
       .JMML.     `"bmmd"'     `"bmmmdPY .AMA.   .AMMA.    .JMML..JMML.

    \b
    TOGA2 - Tool for Ortholog Inference from Genome Alignment
    Mandatory arguments are:
    * REF_2BIT - a path to reference genome assembly in .2bit format;
    * QUERY_2BIT - a path to query genome assembly, also in .2bit format;
    * ALIGNMENT_CHAINS - a path to genome alignment chains, with REF_2BIT as reference and QUERY_2BIT as query. Can be compressed in .gzip format;
    * REF_ANNOTATION_BED - protein-coding gene annotation for the reference genome, in Bed12 format
    """

    __slots__ = TOGA2_SLOTS

    def __init__(
        self,
        ref_2bit: click.Path,
        query_2bit: click.Path,
        chain_file: click.Path,
        ref_annotation: click.Path,
        resume_from: Optional[str],
        halt_at: Optional[str],
        selected_feature_batches: Optional[Union[str, None]],
        selected_preprocessing_batches: Optional[Union[str, None]],
        selected_alignment_batches: Optional[Union[str, None]],
        no_utr_annotation: Optional[bool],
        isoform_file: Optional[click.Path],
        u12_file: Optional[click.Path],
        spliceai_dir: Optional[click.Path],
        min_chain_score: Optional[int],
        min_orthologous_chain_score: Optional[int],
        feature_jobs: Optional[int],
        orthology_threshold: Optional[float],
        single_exon_model: Optional[click.Path],
        multi_exon_model: Optional[click.Path],
        use_long_distance_model: Optional[bool],
        long_distance_model: Optional[click.Path],
        disable_fragment_assembly: Optional[bool],
        orthologs_only: Optional[bool],
        one2ones_only: Optional[bool],
        enable_spanning_chains: Optional[bool],
        annotate_processed_pseudogenes: Optional[bool],
        preprocessing_jobs: Optional[int],
        max_chains_per_transcript: Optional[int],
        max_search_space_size: Optional[int],
        extrapolation_modifier: Optional[int],
        minimal_covered_fraction: Optional[float],
        exon_locus_flank: Optional[int],
        assembly_gap_size: Optional[int],
        cesar_binary: Optional[Union[str, None]],
        memory_bins: Optional[str],
        job_nums_per_bin: Optional[str],
        allow_heavy_jobs: Optional[bool],
        cesar_memory_limit: Optional[int],
        cesar_canon_u2_acceptor: Optional[click.Path],
        cesar_canon_u2_donor: Optional[click.Path],
        cesar_non_canon_u2_acceptor: Optional[click.Path],
        cesar_non_canon_u2_donor: Optional[click.Path],
        cesar_canon_u12_acceptor: Optional[click.Path],
        cesar_canon_u12_donor: Optional[click.Path],
        cesar_non_canon_u12_acceptor: Optional[click.Path],
        cesar_non_canon_u12_donor: Optional[click.Path],
        cesar_first_acceptor: Optional[click.Path],
        cesar_last_donor: Optional[click.Path],
        separate_splice_site_treatment: Optional[bool],
        spliceai_correction_mode: Optional[int],
        min_splice_prob: Optional[float],
        splice_prob_margin: Optional[float],
        intron_gain_check: Optional[bool],
        intron_gain_threshold: Optional[float],
        min_intron_prob_trusted: Optional[bool],
        min_intron_prob_supported: Optional[bool],
        min_intron_prob_unsupported: Optional[bool],
        max_intron_number: Optional[int],
        matrix: Optional[click.Path],
        mask_n_terminal_mutations: Optional[bool],
        disable_missing_stop_search: Optional[bool],
        accepted_loss_symbols: Optional[str],
        skip_gene_trees: Optional[bool],
        max_clique_size: Optional[int],
        use_raxml: Optional[bool],
        orthology_jobs: Optional[int],
        prank_binary: Optional[Union[click.Path, None]],
        tree_binary: Optional[Union[click.Path, None]],
        tree_cpus: Optional[int],
        utr_abs_threshold: Optional[int],
        utr_rel_threshold: Optional[float],
        no_utr_boundary_extrapolation: Optional[bool],
        no_adjacent_utr_extra: Optional[bool],
        fixed_adjacent_utr_extra: Optional[bool],
        link_file: Optional[Union[click.Path, None]],
        ucsc_prefix: Optional[str],
        parallel_strategy: Optional[str],
        nextflow_exec_script: Optional[Union[click.Path, None]],
        max_number_of_retries: Optional[int],
        nextflow_config_dir: Optional[Union[click.Path, None]],
        max_parallel_time: Optional[int],
        cluster_queue_name: Optional[str],
        keep_nextflow_log: Optional[bool],
        ignore_crashed_parallel_batches: Optional[bool],
        legacy_chain_feature_extraction: Optional[bool],
        toga1_compatible: Optional[bool],
        toga1_plus_corrected_cesar: Optional[bool],
        account_for_alternative_frame: Optional[bool],
        output: Optional[click.Path],
        project_name: Optional[str],
        keep_temporary_files: Optional[bool],
        verbose: Optional[bool],
        email: Optional[Union[str, None]],
        mailx_binary: Optional[Union[str, None]],
        fatotwobit_binary: Optional[Union[click.Path, None]],
        twobittofa_binary: Optional[Union[click.Path, None]],
        bigwig2wig_binary: Optional[click.Path],
        bedtobigbed_binary: Optional[Union[click.Path, None]],
        ixixx_binary: Optional[Union[click.Path, None]],
        # version: Optional[bool],
    ) -> None:
        self.v: bool = verbose
        self.project_name: str = project_name
        self.project_id: str = hex_dir_name(self.project_name)

        ## internalized attributes
        self.ref_2bit: click.Path = self._abspath(ref_2bit)
        self.query_2bit: click.Path = self._abspath(query_2bit)
        self.chain_file: click.Path = self._abspath(chain_file)
        self.ref_annotation: click.Path = self._abspath(ref_annotation)

        self.resume_from: str = resume_from
        self.halt_at: str = halt_at
        self.selected_feature_batches: List[str] = (
            selected_feature_batches.split(",")
            if selected_feature_batches is not None
            else None
        )
        self.selected_preprocessing_batches: List[str] = (
            selected_preprocessing_batches.split(",")
            if selected_preprocessing_batches is not None
            else None
        )
        self.selected_alignment_batches: List[str] = (
            selected_alignment_batches.split(",")
            if selected_alignment_batches is not None
            else None
        )
        self.skip_utr: bool = no_utr_annotation

        self.isoform_file: Union[click.Path, None] = self._abspath(isoform_file)
        self.u12_file: Union[click.Path, None] = self._abspath(u12_file)
        self.spliceai_dir: Union[click.Path, None] = self._abspath(spliceai_dir)

        self.min_chain_score: int = min_chain_score
        self.min_orth_chain_score: int = min_orthologous_chain_score

        self.feature_job_num: int = feature_jobs
        self.orthology_threshold: float = orthology_threshold
        self.se_model: click.Path = single_exon_model
        self.me_model: click.Path = multi_exon_model
        self.use_ld_model: bool = long_distance_model
        self.ld_model: click.Path = long_distance_model

        self.disable_fragment_assembly: bool = disable_fragment_assembly
        self.annotate_ppgenes: bool = annotate_processed_pseudogenes
        self.orthologs_only: bool = orthologs_only
        self.one2ones_only: bool = one2ones_only
        self.enable_spanning_chains: bool = enable_spanning_chains

        self.preprocessing_job_num: int = preprocessing_jobs
        self.max_chains_per_transcript: int = max_chains_per_transcript
        self.cesar_memory_limit: float = cesar_memory_limit
        self.max_search_space_size: int = max_search_space_size
        self.extrapolation_modifier: float = extrapolation_modifier
        self.minimal_covered_fraction: float = (
            minimal_covered_fraction if not enable_spanning_chains else 0.0
        )
        self.exon_locus_flank: int = exon_locus_flank
        self.assembly_gap_size: int = assembly_gap_size
        self.cesar_canon_u2_acceptor: click.Path = self._abspath(
            cesar_canon_u2_acceptor
        )
        self.cesar_canon_u2_donor: click.Path = self._abspath(cesar_canon_u2_donor)
        self.cesar_non_canon_u2_acceptor: click.Path = self._abspath(
            cesar_non_canon_u2_acceptor
        )
        self.cesar_non_canon_u2_donor: click.Path = self._abspath(
            cesar_non_canon_u2_donor
        )
        self.cesar_canon_u12_acceptor: click.Path = self._abspath(
            cesar_canon_u12_acceptor
        )
        self.cesar_canon_u12_donor: click.Path = self._abspath(cesar_canon_u12_donor)
        self.cesar_non_canon_u12_acceptor: click.Path = self._abspath(
            cesar_non_canon_u12_acceptor
        )
        self.cesar_non_canon_u12_donor: click.Path = self._abspath(
            cesar_non_canon_u12_donor
        )
        self.cesar_first_acceptor: click.Path = self._abspath(cesar_first_acceptor)
        self.cesar_last_donor: click.Path = self._abspath(cesar_last_donor)
        self.separate_site_treat: bool = separate_splice_site_treatment

        self.min_splice_prob: float = min_splice_prob
        self.splice_prob_margin: float = splice_prob_margin
        self.intron_gain_check: bool = intron_gain_check
        self.min_intron_gain_score: float = intron_gain_threshold
        # self.min_intron_prob_gapped: float = min_intron_prob_gapped
        # self.min_intron_prob_ungapped: float = min_intron_prob_ungapped
        self.min_intron_prob_trusted: bool = min_intron_prob_trusted
        self.min_intron_prob_supported: bool = min_intron_prob_supported
        self.min_intron_prob_unsupported: bool = min_intron_prob_unsupported
        self.max_intron_number: int = max_intron_number
        self.cesar_binary: Union[str, None] = cesar_binary
        self.cesar_memory_bins: str = memory_bins
        self.job_nums_per_bin: str = job_nums_per_bin
        self.allow_heavy_jobs: bool = allow_heavy_jobs
        self.matrix_file: str = self._abspath(matrix)
        self.mask_terminal_mutations: bool = mask_n_terminal_mutations
        self.leave_missing_stop: bool = disable_missing_stop_search
        self.consider_alt_frame: bool = account_for_alternative_frame
        self.spliceai_correction_mode: int = spliceai_correction_mode

        self.accepted_loss_symbols: str = accepted_loss_symbols
        self.skip_tree_resolver: bool = skip_gene_trees
        self.max_clique_size: int = max_clique_size
        self.use_raxml: bool = use_raxml
        self.orth_job_num: int = orthology_jobs
        self.prank_bin: Union[str, None] = prank_binary
        self.tree_bin: Union[str, None] = tree_binary
        self.tree_cpus: int = tree_cpus

        self.utr_abs_threshold: int = utr_abs_threshold
        self.utr_rel_threshold: float = utr_rel_threshold
        self.no_utr_extrapolation: bool = no_utr_boundary_extrapolation
        self.no_adjacent_utrs: bool = no_adjacent_utr_extra
        self.fixed_adjacent_utrs: bool = fixed_adjacent_utr_extra

        self.ref_link_file: Union[str, None] = link_file
        self.ucsc_prefix: str = ucsc_prefix

        self.email: Union[str, None] = email
        self.mailx_binary: Union[click.Path, None] = mailx_binary

        self.fatotwobit_binary: Union[click.Path, None] = fatotwobit_binary
        self.twobittofa_binary: Union[click.Path, None] = twobittofa_binary
        self.bigwig2wig_binary: click.Path = bigwig2wig_binary
        self.bedtobigbed_binary: Union[click.Path, None] = bedtobigbed_binary
        self.ixixx_binary: Union[click.Path, None] = ixixx_binary

        self.parallel_strategy: str = parallel_strategy
        self.max_number_of_retries: int = max_number_of_retries
        self.nextflow_exec_script: str = nextflow_exec_script
        self.local_executor: str = (
            parallel_strategy == "local"
        )  # self.nextflow_config_dir is not None ## TODO: Add config dir content check
        self.max_parallel_time: int = max_parallel_time
        self.keep_nextflow_log: bool = keep_nextflow_log
        self.cluster_queue_name: str = cluster_queue_name
        self.ignore_crashed_parallel_batches: bool = ignore_crashed_parallel_batches
        self.legacy_chain_feature_extraction: bool = legacy_chain_feature_extraction
        self.parallel_process_names: List[str] = []

        self.output: str = Path(
            output if output else dir_name_by_date("toga2_run")
        ).absolute()
        self.keep_tmp: bool = keep_temporary_files

        ## benchmarking flags
        self.toga1: bool = toga1_compatible
        self.toga1_plus_cesar: bool = toga1_plus_corrected_cesar

        ## input genome file status flags; OBSOLETE??
        # self.ref_is_2bit: bool = False
        # self.ref_is_gzipped: bool = False
        # self.query_is_2bit: bool = False
        # self.query_is_gzipped: bool = False

        ## failed batch collections
        self.failed_feature_batches: List[str] = []
        self.failed_preprocessing_batches: List[str] = []
        self.failed_alignment_batches: List[str] = []
        self.failed_orthology_batches: List[str] = []

        ## preset Nextflow log files
        self.nextflow_config_files: Dict[str, Union[str, None]] = {}

        ## directory structure
        ## first-level
        self.tmp: str = os.path.join(self.output, "tmp")
        self.meta: str = os.path.join(self.output, "meta")
        # self.res: str = os.path.join(self.output, 'results')
        self.logs: str = os.path.join(self.output, "logs")
        self.nextflow_dir: str = os.path.join(self.output, "nextflow")
        self.nextflow_config_dir: str = (
            self.nextflow_dir if nextflow_config_dir is None else nextflow_config_dir
        )
        self.ucsc_dir: str = os.path.join(self.output, "ucsc_browser_files")
        self.arg_file: str = os.path.join(
            self.logs, f"project_args_{self.project_id}.tsv"
        )
        self.log_file: str = os.path.join(self.logs, f"{self.project_id}.log")
        self.failed_batches_file: str = os.path.join(
            self.logs, f"failed_batches_{self.project_id}.tsv"
        )

        ## temporary subdirectories
        self.input_data: str = os.path.join(self.tmp, "input_data")
        self.feature_job_dir: str = os.path.join(self.tmp, "feature_extraction_jobs")
        self.feature_data_dir: str = os.path.join(self.tmp, "feature_extraction_input")
        self.feature_res_dir: str = os.path.join(self.tmp, "feature_extraction_results")
        self.classification_dir: str = os.path.join(
            self.tmp, "projection_classification"
        )
        self.preprocessing_job_dir: str = os.path.join(
            self.tmp, "cesar_preprocessing_jobs"
        )
        self.preprocessing_job_dir: str = os.path.join(
            self.tmp, "cesar_preprocessing_jobs"
        )
        self.preprocessing_res_dir: str = os.path.join(
            self.tmp, "cesar_preprocessing_res"
        )
        self.alignment_job_dir: str = os.path.join(self.tmp, "cesar_alignment_jobs")
        self.alignment_res_dir: str = os.path.join(self.tmp, "cesar_alignment_res")
        self.orthology_resolution_dir: str = os.path.join(
            self.tmp, "orthology_resolution"
        )
        self.orthology_job_dir: str = os.path.join(
            self.tmp, "orthology_resolution_jobs"
        )
        self.orthology_input_dir: str = os.path.join(
            self.tmp, "orthology_resolution_input"
        )
        self.orthology_res_dir: str = os.path.join(self.tmp, "orthology_resolution_res")
        self.vis_input_dir: str = os.path.join(self.tmp, "bed_files_for_browsers")
        self.finalized_output_dir: str = os.path.join(
            self.tmp, "finalized_output_files"
        )

        ## meta subdirectories
        self.rejection_dir: str = os.path.join(self.tmp, "rejection_logs")

        ## input file copies and indices
        self.bed_file_copy: str = os.path.join(
            self.input_data, "reference_annotation.bed"
        )
        self.ref_cds_unfilt: str = os.path.join(
            self.input_data, "reference_annotation_raw_cds.bed"
        )
        self.prefiltered_transcripts: str = os.path.join(
            self.rejection_dir, "prefiltered_transcripts.tsv"
        )
        self.chain_file_copy: str = os.path.join(
            self.input_data, "genome_alignment.chain"
        )
        self.chain_index: str = os.path.join(self.input_data, "genome_alignment.bst")
        self.chain_index_txt: str = os.path.join(
            self.input_data, "genome_alignment.chain_ID_position"
        )
        self.full_bed_hdf5: str = os.path.join(
            self.input_data, "reference_annotation.hdf5"
        )
        self.cds_bed_file: str = os.path.join(
            self.input_data, "reference_annotation_cds.bed"
        )
        self.cds_bed_hdf5: str = os.path.join(
            self.input_data, "reference_annotation_cds.hdf5"
        )
        self.u12_hdf5: str = os.path.join(self.input_data, "u12_data.hdf5")
        self.ref_contig_size_file: str = os.path.join(
            self.input_data, "reference_contig_sizes.tsv"
        )
        self.query_contig_size_file: str = os.path.join(
            self.input_data, "query_contig_sizes.tsv"
        )
        self.aa_hdf5: str = os.path.join(self.input_data, "protein_sequences.hdf5")
        self.annot_dir: str = os.path.join(self.tmp, "annotation_raw")
        self.all_discarded_projections: str = os.path.join(
            self.annot_dir, "all_discarded_projections.txt"
        )
        self.cds_fasta_tmp: str = os.path.join(self.annot_dir, "nucleotide.fa")
        self.prot_fasta_tmp: str = os.path.join(self.annot_dir, "protein.fa")
        self.orth_resolution_raw: str = os.path.join(
            self.annot_dir, "orthology_classification.tsv"
        )
        self.query_annotation_filt: str = os.path.join(
            self.annot_dir, "query_annotation.bed"
        )
        self.query_genes_raw: str = os.path.join(self.annot_dir, "query_genes.tsv")
        self.query_genes_bed_raw: str = os.path.join(self.annot_dir, "query_genes.bed")
        self.query_genes_for_gtf: str = os.path.join(
            self.annot_dir, "query_genes.for_gtf.tsv"
        )

        ## joblists
        self.feature_extraction_joblist: str = os.path.join(
            self.feature_job_dir, "joblist"
        )
        self.cesar_preprocess_joblist: str = os.path.join(
            self.preprocessing_job_dir, "joblist_preprocess"
        )
        self.cesar_job_list_summary: str = os.path.join(
            self.alignment_job_dir, "joblist_description.tsv"
        )
        self.orth_resolution_joblist: str = os.path.join(
            self.orthology_job_dir, "joblist"
        )

        ## provisional and meta files
        self.feature_table: str = os.path.join(self.meta, "projection_features.tsv")
        self.tr2chain_classes: str = os.path.join(
            self.meta, "trans_to_chain_classes.tsv"
        )
        self.feature_rejection_log: str = os.path.join(
            self.rejection_dir, "rejected_at_feature_extraction.tsv"
        )
        self.class_rejection_log: str = os.path.join(
            self.rejection_dir, "rejected_at_classification.tsv"
        )
        self.preprocessing_rejection_log: str = os.path.join(
            self.rejection_dir, "rejected_at_preprocessing.tsv"
        )
        self.all_deprecated_projs: str = os.path.join(
            self.rejection_dir, "all_deprecated_projections.txt"
        )
        self.fragmented_projection_list: str = os.path.join(
            self.meta, "fragmented_projections.tsv"
        )
        self.preprocessing_report: str = os.path.join(
            self.meta, "memory_requirements.tsv"
        )
        # self.spanning_chain_precedence_file: str = os.path.join(
        #     self.meta, 'spanning_chains_ref_spans.tsv'
        # )
        self.rejected_by_graph: str = os.path.join(
            self.orthology_resolution_dir, "rejected_by_graph_reduction.tsv"
        )
        self.weak_ortholog_names: str = os.path.join(
            self.orthology_resolution_dir, "rejected_projection_names.txt"
        )
        self.discarded_overextended_projections: str = os.path.join(
            self.meta, "discarded_overextended_projections.txt"
        )
        self.missing_transcripts: str = os.path.join(
            self.orthology_resolution_dir, "unclassified_transcripts.txt"
        )
        self.resolved_leaves_file: str = os.path.join(
            self.meta, "resolved_tree_pairs.tsv"
        )
        self.unresolved_clades_file: str = os.path.join(
            self.meta, "unresolved_tree_clades.txt"
        )
        self.temporary_orth_report: str = os.path.join(
            self.orthology_resolution_dir, "orthology_classification.tsv"
        )
        self.rejected_at_tree_step: str = os.path.join(
            self.meta, "rejected_at_orthology_step.tsv"
        )
        self.one2zero_genes: str = os.path.join(self.meta, "one2zero_genes.txt")
        self.spanning_chain_coords: str = os.path.join(
            self.meta, "spanning_chain_ref_coords.tsv"
        )
        self.paralog_report: str = os.path.join(
            self.meta, "paralogous_projections_to_align.tsv"
        )
        self.processed_pseudogene_report: str = os.path.join(
            self.meta, "processed_pseudogene_projections_to_align.tsv"
        )
        self.redundant_paralogs: str = os.path.join(self.meta, "redundant_paralogs.txt")
        self.redundant_ppgenes: str = os.path.join(
            self.meta, "redundant_processed_pseudogenes.txt"
        )
        self.discarded_proj_bed: str = os.path.join(
            self.meta, "discarded_projections.bed"
        )
        self.redundancy_rejection_log: str = os.path.join(
            self.rejection_dir, "rejected_by_redundancy.tsv"
        )
        self.alignment_rejection_log: str = os.path.join(
            self.rejection_dir, "rejected_at_alignment.tsv"
        )
        self.gene_inference_rejection_log: str = os.path.join(
            self.rejection_dir, "rejected_at_gene_inference.tsv"
        )
        self.aggr_ucsc_stub: str = os.path.join(
            self.vis_input_dir, "ucsc_bed_stub.bed23"
        )
        self.decor_stub: str = os.path.join(self.vis_input_dir, "decorator.bb")
        self.loss_summary_extended: str = os.path.join(
            self.meta, "loss_summary_extended.tsv"
        )

        ## output files and directories
        self.transcript_meta: str = os.path.join(self.meta, "transcript_meta.tsv")
        self.splice_sites: str = os.path.join(self.meta, "splice_sites.tsv")
        self.gained_intron_summary: str = os.path.join(
            self.meta, "gained_intron_summary.tsv"
        )
        self.splice_site_shifts: str = os.path.join(self.meta, "splice_site_shifts.tsv")
        self.selenocysteine_codons: str = os.path.join(
            self.meta, "selenocysteine_codons.tsv"
        )
        self.query_annotation_raw: str = os.path.join(
            self.meta, "query_annotation.with_discarded_exons.bed"
        )
        self.query_exon_meta: str = os.path.join(self.meta, "exon_meta.tsv")
        self.tree_summary_table: str = os.path.join(
            self.meta, "gene_tree_job_summary.tsv"
        )
        # self.orthology_results_dir: str = os.path.join(
        #     self.tmp, 'orthology_resolution'
        # )

        ## ultimate output files
        self.pred_scores: str = os.path.join(self.output, "orthology_scores.tsv")
        self.query_annotation_final: str = os.path.join(
            self.output, "query_annotation.bed"
        )
        self.query_annotation_with_utrs: str = os.path.join(
            self.output, "query_annotation.with_utrs.bed"
        )
        self.query_gtf: str = os.path.join(self.output, "query_annotation.gtf")
        self.aa_fasta: str = os.path.join(self.output, "protein_aln.fa")
        self.cds_fasta: str = os.path.join(self.output, "nucleotide.fa")
        self.codon_fasta: str = os.path.join(self.output, "codon_aln.fa")
        self.exon_fasta: str = os.path.join(self.output, "exon_aln.fa")
        self.exon_2bit: str = os.path.join(self.output, "exon_seqs.2bit")
        self.mutation_report: str = os.path.join(
            self.output, "inactivating_mutations.tsv"
        )
        self.processed_pseudogene_annotation: str = os.path.join(
            self.output, "processed_pseudogenes.bed"
        )
        self.gene_loss_summary: str = os.path.join(self.output, "loss_summary.tsv")
        self.final_rejection_log: str = os.path.join(self.output, "rejected_items.tsv")
        self.pseudogene_annotation: str = os.path.join(
            self.output, "processed_pseudogenes.bed"
        )
        self.prot_fasta: str = os.path.join(self.output, "protein.fa")
        self.query_genes: str = os.path.join(self.output, "query_genes.tsv")
        self.query_genes_bed: str = os.path.join(self.output, "query_genes.bed")
        self.orth_resolution_report: str = os.path.join(
            self.output, "orthology_classification.tsv"
        )
        self.summary: str = os.path.join(self.output, "summary.txt")
        self.decoration_track: str = os.path.join(
            self.ucsc_dir, f"{self.ucsc_prefix}.decorator.bb"
        )
        self.aa_gzip: str = self.aa_fasta + ".gz"
        self.cds_gzip: str = self.cds_fasta + ".gz"
        self.codon_gzip: str = self.codon_fasta + ".gz"
        self.exon_gzip: str = self.exon_fasta + ".gz"
        self.prot_gzip: str = self.prot_fasta + ".gz"
        self.gtf_gzip: str = self.query_gtf + ".gz"
        self.splice_sites_gzip: str = self.splice_sites + "gz"
        self.exon_meta_gzip: str = self.query_exon_meta + ".gz"
        self.transcript_meta_gzip: str = self.transcript_meta + ".gz"

        ## script location attributes
        ## NOTE: Most of these scripts are now obsolete since the modules are imported directly to the mothership

        self.CHAIN_FILTER_SCRIPT: str = os.path.join(
            LOCATION, "src", "rust", "target", "release", "chain_filter"
        )
        self.INDEX_CHAIN_SCRIPT: str = os.path.join(
            PYTHON_DIR, "modules", "chain_bst_index.py"
        )
        self.REF_BED_FILTER: str = os.path.join(
            PYTHON_DIR, "modules", "filter_ref_bed.py"
        )
        self.CDS_TRACK_SCRIPT: str = os.path.join(
            LOCATION, "src", "rust", "target", "release", "bed12ToFraction"
        )
        self.REF_BED_TO_HDF5: str = os.path.join(
            PYTHON_DIR, "modules", "bed_hdf5_index.py"
        )
        self.U12_TO_HDF5_SCRIPT: str = os.path.join(
            PYTHON_DIR, "modules", "intronIC_to_hdf5.py"
        )
        self.CONTIG_SIZE_SCRIPT: str = os.path.join(
            PYTHON_DIR, "get_contig_sizes.py"
        )  ## TODO: Alejandro must have a Rust implementation
        self.FEATURE_EXTRACTOR: str = os.path.join(
            LOCATION, "src", "rust", "target", "release", "feature_extraction"
        )
        self.MODEL_TRAINER: str = os.path.join(PYTHON_DIR, "train_model.py")
        self.FINAL_RESOLVER_SCRIPT: str = os.path.join(
            LOCATION, "modules", "final_orthology_aggregator.py"
        )
        self.FASTA_FILTER_SCRIPT: str = os.path.join(
            LOCATION, "src", "rust", "target", "release", "filter_fasta"
        )
        self.UTR_PROJECTOR_SCRIPT: str = os.path.join(
            LOCATION, "src", "rust", "target", "release", "utr_projector"
        )
        self.DECORATOR_SCRIPT: str = os.path.join(
            LOCATION, "src", "rust", "target", "release", "add_decorations"
        )
        self.GTF_SCRIPT: str = os.path.join(
            LOCATION, "bed2gtf", "target", "release", "bed2gtf"
        )
        self.SCHEMA_FILE: str = os.path.join(LOCATION, "supply", "bb_schema_toga2.as")
        self.DECOR_SCHEMA_FILE: str = os.path.join(LOCATION, "supply", "decoration.as")

        self.run()

    def run(self) -> None:
        """
        Executes the TOGA 2.0 routine
        """
        self._echo("Initializing TOGA 2")
        ## Step 0: Output directory creation, input preparation, and sanity checks
        ## create the output directory
        self._echo("Creating working directory")
        self.create_wd()
        self._to_log("Working directory created")
        self._to_log("Writing project metadata")
        self.write_project_meta()
        self._to_log(f"Project metadata dumped at {self.arg_file}")

        ## check the input arguments
        # self._echo('Checking input arguments')
        self._to_log("Checking input arguments")
        self.check_arguments()
        # self._echo('All settings are correct')
        self._to_log("All settings are correct")

        ## check the availability of all the necessary binaries
        # self._echo('Checking third-part dependencies')
        self._to_log("Checking third-part dependencies")
        self.check_binaries()
        # self._echo('All dependencies were found')
        self._to_log("All dependencies were found")

        ## if SpliceAI directory was provided, check its contents
        self.check_spliceai_files()

        ## if a custom Nextflow executor script was not provided,
        ## generate one from the boilerplate, substituting the maxRetrie parameter
        if self.nextflow_exec_script is None:
            self._to_log("Generating a Nextflow execution master script")
            self._generate_nf_script()
            self._to_log(
                "Nextflow execution script saved at %s" % self.nextflow_exec_script
            )

        ## if a path to custom Nextflow configuration files was provided,
        ## check the directory completeness
        self._check_nextflow_configs()

        ## check the format consistency of all the input files
        # self.check_ref_bed_file()
        # self.check_u12_file()
        # self.check_isoform_file_file()

        ## create a sanity checker instance
        self.result_checker: ResultChecker = ResultChecker(
            self.logger,
            self.output,
            self.isoform_file,
            self.orthology_threshold,
            self.accepted_loss_symbols,
        )

        ## create the necessary links, indices, and HDF5 storage files
        if self._execute_step("setup"):
            self._to_log("Preparing input data")
            self.prepare_data()
            self._to_log("All input data successfully checked and handled")

        ## showdowm
        self._to_log("Running TOGA main procedure")
        ## Step 1: Extract chain features
        if self._execute_step("feature_extraction"):
            if not self.legacy_chain_feature_extraction:
                self._to_log("Exracting projection features for further classification")
                self.extract_chain_features()
                pass
            else:
                ## 1a: Schedule chain jobs
                if self.selected_feature_batches is None:
                    self._to_log("Scheduling projection feature extraction jobs")
                    self.schedule_feature_jobs()
                else:
                    self._to_log(
                        "Preparing selected feature extraction batches for parallel run"
                    )
                    self._schedule_selected_batches("feature_extraction")
                self._to_log("Feature extraction jobs successfully scheduled")
                ## 1b: Run the feature extraction jobs
                self._to_log("Extracting projection features (parallel step)")
                self.run_feature_extraction_jobs()
                self._to_log("All projection feature jobs finished")
                ## 1c: Aggregate chain feature data and create a classification dataset
                self._to_log("Aggregating projection features")
                self.create_feature_dataset()
                self._to_log("Feature dataset successfully prepared")
        else:
            if self.halt_at == "feature_extraction":
                self.notify_on_completion("feature_extraction")
                self._exit(
                    "Finish pipeline before feature extraction step as suggested"
                )
            self._to_log("Skipping feature extraction step as suggested")

        ## Step 2: Classify chain-transcript pairs in terms of orthology
        if self._execute_step("classification"):
            self._to_log("Classifying projections")
            self.classify_chains()
            self._to_log("Projection successfully classified")
        else:
            if self.halt_at == "classification":
                self.notify_on_completion("classification")
                self._exit(
                    "Finish pipeline before projection classification step as suggested"
                )
            self._to_log("Skipping projection classification step as suggested")
        self._sanity_check(self.result_checker.check_classification())

        if self._execute_step("preprocessing"):
            ## Step 2.5: Unless explicitly disabled, stitch fragmented projections
            if not self.disable_fragment_assembly:
                self._to_log("Recovering fragmented projections")
                self.recover_fragmented_projections()
                # self._echo('Fragmented projections successfully recovered')
                self._to_log("Fragmented projections successfully recovered")

            ## Step 3: Prepare and run CESAR preprocessing jobs
            ## prepare preprocessing jobs
            if self.selected_preprocessing_batches is None:
                self._to_log("Scheduling preprocessing jobs", "info")
                self.schedule_preprocessing_jobs()
            else:
                self._to_log(
                    "Preparing selected preprocessing batches for parallel run"
                )
                self._schedule_selected_batches("preprocessing")
            self._to_log("Preprocessing jobs scheduled", "info")

            ## run preprocessing jobs
            self._to_log("Running preprocessing jobs (parallel step)")
            self.run_preprocessing_jobs()
            self._to_log("Preprocessing jobs finished")
        else:
            if self.halt_at == "preprocessing":
                self.notify_on_completion("preprocessing")
                self._exit(
                    "Finish pipeline before CESAR preprocessing step as suggested"
                )
            self._to_log("Skipping CESAR preprocessing step as suggested")

        ## Step 4: Aggregate preprocessing results
        if self._execute_step("aggregate_preprocessing_res"):
            self._to_log("Aggregating preprocessing results")
            self.aggregate_preprocessing_res()
        else:
            if self.halt_at == "aggregate_preprocessing_res":
                self.notify_on_completion("aggregate_preprocessing_res")
                self._exit(
                    "Finish aggregatting preprocessing results step as suggested"
                )
            self._to_log(
                "Skipping aggregatting preprocessing results step as suggested"
            )
        self._sanity_check(self.result_checker.check_preprocessing())

        ## Step 4: Prepare and run CESAR alignment & postprocessing jobs
        if self._execute_step("alignment"):
            ## prepare alignment jobs
            if self.selected_alignment_batches is None:
                self._to_log("Scheduling CESAR alignment jobs")
                self.schedule_alignment_jobs()
            else:
                self._to_log("Preparing selected alignment batches for parallel run")
                self._schedule_selected_batches("alignment")
            self._to_log("Alignment jobs scheduled", "info")

            ## then run them
            self._to_log("Running alignment jobs (parallel step)")
            self.run_alignment_jobs()
            self._to_log("Alignment step complete")
        else:
            if self.halt_at == "alignment":
                self.notify_on_completion("alignment")
                self._exit(
                    "Finishing pipeline before CESAR alignment step as suggested"
                )
            self._to_log("Skipping CESAR alignment step as suggested")

        ## Step 5: Aggregate alignment/postprocessing results
        if self._execute_step("aggregate_cesar_res"):
            self._to_log("Aggregating alignment results")
            self.aggregate_cesar_results()
            self._to_log("CESAR step results successfully aggregated")
        else:
            if self.halt_at == "aggregate_cesar_res":
                self.notify_on_completion("aggregate_cesar_res")
                self._exit(
                    "Finishing pipeline before alignment results aggregation step as suggested"
                )
            self._to_log("Skipping alignment results aggregation step as suggested")

        ## Step 6: Infer query gene structure
        if self._execute_step("gene_inference"):
            ## 6a: Aggregate rejection reports from all the previous steps
            self._to_log("Aggregating rejection reports")
            self.aggregate_rejection_reports()
            self._to_log(
                "Rejected items from all the finished steps are aggregated at %s"
                % (self.final_rejection_log),
                "info",
            )
            ## 6b: Infer query genes as the step name suggests
            self._to_log("Inferring query genes")
            self.infer_query_genes()
            self._to_log("Query genes successfully inferred")
            ## 6c: If processed pseudogenes were not considered in the previous steps,
            ## prepare a simple BED9 track
            if not self.annotate_ppgenes:
                self._to_log("Preparing a processed pseudogene BED track")
                self.prepare_pseudogene_track()
                self._to_log("Pseudogene track successfully produced")
        else:
            if self.halt_at == "gene_inference":
                self.notify_on_completion("gene_inference")
                self._exit(
                    "Finishing pipeline before query gene inference step as suggested"
                )
            self._to_log("Skipping gene loss summary step as suggested")

        ## Step 6: Summarise ortholog presence status on projection, transcript,
        ## and (if requested) gene levels
        if self._execute_step("loss_summary"):
            ## 6b: Summarise loss data
            self._to_log("Summarizing transcript/gene conservation data")
            self.loss_status_summary()
            # self._echo('Conservation data successfully summarized')
            self._to_log("Conservation data successfully summarized")
        else:
            if self.halt_at == "loss_summary":
                self.notify_on_completion("loss_summary")
                self._exit(
                    "Finishing pipeline before gene loss summary step as suggested"
                )
            self._to_log("Skipping gene loss summary step as suggested")
        self._sanity_check(self.result_checker.check_loss_summary())

        ## Step 7: Resolve orthology status for all reference genes
        if self._execute_step("orthology"):
            ## resolve orthology relationships with a graph-based method
            if not self.skip_tree_resolver:
                self._to_log(
                    "Converting protein annotation FASTA file into HDF5 format"
                )
                self.convert_fasta_to_hdf5()
                self._to_log("HDF5 preparation is complete")
            self._to_log("Resolving orthology relationships")
            self._to_log("Running initial step (graph-based resolution)")
            self.resolve_orthology_initial()
            self._to_log("Graph-based step successfully finished")
            ## aggregate the results

            ## if requested, run fine orthology resolution jobs
            if not self.skip_tree_resolver:
                self._to_log(
                    "Running fine orthology resolution jobs (parallel step)",
                )
                self.run_tree_jobs()
                self._to_log("Tree jobs finished", "info")
                self._to_log("Updating orthology estimates with tree-inferred data")
            self._to_log("Orthology inference step complete", "info")
        else:
            if self.halt_at == "orthology":
                self.notify_on_completion("orthology")
                self._exit(
                    "Finishing pipeline before orthology inference step as suggested"
                )
            self._to_log("Skipping orthology inference step as suggested")
        if self.skip_tree_resolver:
            self._sanity_check(self.result_checker.check_orthology_resolution())

        ## Step 8: If tree-based resolution was requested, summarize its results
        ## and adjust orthology resolution accordingly
        if self._execute_step("summarize_trees") and not self.skip_tree_resolver:
            self._to_log("Adding gene tree step results to orthology resolution")
            self.resolve_orthology_final()
            self._to_log("Creating a summary table for the gene tree resolution step")
            self.gene_tree_summary()
            self._to_log("Tree-based orthology inference step complete")
            pass
        else:
            if self.halt_at == "summarize_trees":
                self.notify_on_completion("summarize_trees")
                self._exit(
                    "Finishing pipeline before adding gene tree data to orthology inference as suggested"
                )
            else:
                self._to_log(
                    "Skipping adding gene tree data to orthology inference as suggested"
                )
        if not self.skip_tree_resolver:
            self._sanity_check(self.result_checker.check_orthology_resolution())

        ## Step 9: Finalize the output
        if self._execute_step("finalize"):
            # self._to_log('Creating exon FASTA HDF5 storage for SLEASY')
            self._to_log("Aggregatig deprecated projection lists")
            self.deprecated_projection_file()
            self._to_log("Filtering output BED file")
            self.filter_final_bed_files()
            if self.skip_utr:
                self._to_log("Skipping the UTR annotation step as suggested")
            else:
                self._to_log(
                    "Annotating UTR projections and adding them to the output Bed and BigBed files"
                )
                self.annotate_utrs()
            self._to_log("Creating exon 2bit file for SLEASY")
            self.convert_exon_fasta()
            self._to_log("Creating query nucleotide and protein Fasta files")
            self.produce_final_sequence_files()
            self._to_log("Compressing FASTA files from the output")
            self.gzip_heavy_files()
            self._to_log("Filtering final gene loss summary file")
            self.filter_loss_file()
            self._to_log("Finalizing naming notation for query genes")
            self.rename_query_genes()
            self._to_log("Creating GTF version of the final annotation")
            self.create_gtf()
            ## before moving to the final steps, record the failed batches
            self._to_log("Recording failed batches if any found")
            self.write_failed_batches()
        else:
            if self.halt_at == "finalize":
                self.notify_on_completion("finalize")
                self._exit(
                    "Finishing pipeline befiore output finalizing step as suggested"
                )
            self._to_log("Skipping output finalizing step as suggested")

        ## Step 10: Prepare UCSC browser input files
        if self._execute_step("ucsc_report"):
            self._to_log("Preparing BigBed track for UCSC Genome Browser")
            self.prepare_bigbed_track()
            self._to_log("UCSC Genome Browser track successfully created")
        else:
            if self.halt_at == "ucsc_report":
                self.notify_on_completion("ucsc_report")
                self._exit(
                    "Finishing pipeline before UCSC report preparation as suggested"
                )
            self._to_log("Skipping UCSC report preparation step as suggested")

        ## Step 11: Final touch
        ## Drop the summary
        self.get_summary()

        ## If requested, clean the temporary directory
        self._to_log("TOGA2 pipeline finished, cleaning up the temporary data")
        self.cleanup()

        ## If e-mail notification was requested, notify the user on successful pipeline run
        self.notify_on_completion("all")

        ## So long and thanks for the fish!
        self._to_log("TOGA2 pipeline successfully finished")

    def set_logging(self) -> None:
        """
        Sets up logging system for a TogaMain instance
        """
        self.logger: logging.Logger = logging.getLogger(self.project_id)
        file_handler: logging.FileHandler = logging.FileHandler(
            self.log_file, mode="a", encoding=Constants.UTF8
        )
        file_handler.setFormatter(Constants.FORMATTER)
        self.logger.addHandler(file_handler)
        if self.v:
            console_handler: logging.StreamHandler = logging.StreamHandler()
            console_handler.setFormatter(Constants.FORMATTER)
            self.logger.addHandler(console_handler)
        self.logger.propagate = False

    def _to_log(self, msg: str, level: str = "info") -> None:
        """
        Adds the message to the TOGA2 log
        """
        getattr(self.logger, level)(msg)

    def _die(self, msg: str) -> None:
        """Error-exit with a given message"""
        if self.email is not None:
            self._email(
                Constants.CRASH_HEADER.format(self.project_id),
                Constants.CRASH_EMAIL.format(self.project_id, self.output, msg),
            )
        super()._die(msg)

    def _abspath(self, path: Union[str, None]) -> Union[str, None]:
        """Checks whether a path is absolute, prepends root prefix if not"""
        if path is None:
            return path
        if os.path.isabs(path):
            return path
        return os.path.abspath(path)

    def _execute_step(self, step: str):
        """
        Defines whether the current step is to be executed based on 'resume' and 'halt' options
        """
        if step not in Constants.RESUME_ORDER:
            self._die("Improper step name provided")
        regular_start: bool = self.resume_from == "all"
        regular_finish: bool = self.halt_at == "all"
        step: int = Constants.RESUME_ORDER[step]
        resume_step: int = Constants.RESUME_ORDER[self.resume_from]
        halt_step: int = Constants.RESUME_ORDER[self.halt_at]
        exec_started: bool = resume_step <= step or regular_start
        exec_not_finished: bool = step < halt_step or regular_finish
        return exec_started and exec_not_finished

    def write_project_meta(self) -> None:
        """ """
        with open(self.arg_file, "w") as h:
            h.write(f"version\t{__version__}\n")
            for arg, option in TOGA2_SLOT2ARG.items():  ## TODO: Double-check the number
                arg_value: str = getattr(self, arg)
                value: str = (
                    arg_value.name
                    if type(arg_value) is click.utils.LazyFile
                    else None
                    if arg_value == ""
                    else arg_value
                )
                h.write("\t".join(map(str, (option, value))) + "\n")

    def _rsync(self, from_: str, to_: str) -> None:
        """
        Synchonizes local object with a remote instance
        """
        cmd: str = f"rsync -a {from_} {to_}"
        _ = self._exec(cmd, "Symbolic link creation failed with the following error:")

    def _get_manager(self) -> ParallelJobsManager:
        """
        Creates a parallel process manager instance according to the select
        parallel process strategy
        """
        if self.parallel_strategy == "para":
            strategy = ParaStrategy()
        elif self.parallel_strategy == "custom":
            strategy = CustomStrategy()
        else:
            strategy = NextflowStrategy()
        jobs_manager: ParallelJobsManager = ParallelJobsManager(strategy)
        return jobs_manager

    def _run_parallel_process(
        self,
        project_name: str,
        project_path: str,
        joblist: str,
        queue: str,
        wait_for_process: bool = True,
        memory_limit: int = 6.5,
        cpu: int = 1,
        process_num: int = 1000,
        step: str = "",
        config_file: Union[str, None] = None,
    ) -> None:
        """
        A general method for running parallel steps
        """
        job_manager: ParallelJobsManager = self._get_manager()
        manager_data: Dict[str, str] = {
            "project_name": project_name,
            "project_path": project_path,
            "logs_dir": project_path,
            "nextflow_dir": self.nextflow_dir,
            "NF_EXECUTE": self.nextflow_exec_script,
            "keep_nf_logs": self.keep_nextflow_log,
            # 'nexflow_config_file': nextflow_config,
            "temp_wd": self.tmp,
            "queue_name": self.cluster_queue_name,
            "logger": self.logger,
        }
        if config_file is not None:
            manager_data["nextflow_config_file"] = config_file
            manager_data["local_executor"] = False
            manager_data["nextflow_config_dir"] = self.nextflow_config_dir
        else:
            manager_data["local_executor"] = self.local_executor
            manager_data["nextflow_config_dir"] = self.nextflow_dir
        try:
            job_manager.execute_jobs(
                joblist,
                manager_data,
                project_name,
                wait=wait_for_process,
                memory_limit=memory_limit,
                queue_name=queue,
                clean=self.keep_tmp,
                cpu=cpu,
                process_num=process_num,
                executor=self.parallel_strategy,
            )
            if not wait_for_process:
                return job_manager
            else:
                iteration: int = 1
                while job_manager.check_status() is None:
                    duration: int = iteration * Constants.ITER_DURATION
                    self._to_log(
                        "Polling %s jobs, iteration %i; already waiting %i seconds"
                        % (step, iteration, duration)
                    )
                if job_manager.check_status() != 0:
                    self._to_log(
                        "Some of the monitored parallel processes at %s step died"
                        % step,
                        "warning",
                    )
        except KeyboardInterrupt:
            # TogaUtil.terminate_parallel_processes([job_manager])
            self._to_log("Aborting the parallel step")
            self._terminate_parallel_processes([job_manager])
            self._exit()

    def _terminate_parallel_processes(self, managers: ParallelJobsManager) -> None:
        """
        Given a list of parallel job managers,
        kills both the cluster jobs and the governing scheduler
        """
        for manager in managers:
            manager.terminate_process()

    def _monitor_jobs(self, jobs_managers: List[ParallelJobsManager], step: str):
        """Monitors parallel jobs if many batches run simultaneously."""
        self._to_log("Polling cluster jobs")
        iter_num: int = 0
        while True:  # Run until all jobs are done (or crashed)
            all_done: bool = True  # default val, re-define if something is not done
            for job_manager in jobs_managers:
                # check if each process is still running
                rc = job_manager.check_status()
                if rc is None:
                    all_done = False
            if all_done:
                self._to_log("All parallel jobs done ###")
                break
            else:
                if iter_num:
                    self._to_log(
                        "Polling iteration %i; already waiting %i seconds."
                        % (iter_num, iter_num * Constants.ITER_DURATION)
                    )
                time.sleep(Constants.ITER_DURATION)
                iter_num += 1

        if any(jm.check_status() != 0 for jm in jobs_managers):
            self._to_log(
                "Some of the monitored parallel processes at %s step died" % step,
                "warning",
            )

    def _schedule_selected_batches(self, step: str) -> None:
        """Prepares a short job list for selected batches for a given step"""
        jobfiles_to_execute: Dict[int, List[str]] = defaultdict(list)
        joblist2jobs: Dict[str, List[str]] = defaultdict(list)
        step2joblist: Dict[str, str] = {
            "feature_extraction": "feature_extraction_joblist",
            "preprocessing": "cesar_preprocess_joblist",
        }
        if step == "feature_extraction":
            batch_ids: List[str] = self.selected_alignment_batches
            job_dir: str = self.feature_job_dir
            output_dir: str = self.feature_res_dir
            joblists: Dict[int, str] = {0: self.__getattribute__(step2joblist[step])}
        elif step == "preprocessing":
            batch_ids: List[str] = self.selected_preprocessing_batches
            job_dir: str = self.preprocessing_job_dir
            output_dir: str = self.preprocessing_res_dir
            joblists: Dict[int, str] = {0: self.__getattribute__(step2joblist[step])}
        elif step == "alignment":
            batch_ids: List[str] = self.selected_alignment_batches
            job_dir: str = self.alignment_job_dir
            output_dir: str = self.alignment_res_dir
            if not os.path.exists(self.cesar_job_list_summary):
                self._die(
                    (
                        "Job list summary %s for the alignment step does not exists; "
                        "consider re-running the alignment step altogether"
                    )
                    % self.cesar_job_list_summary
                )
            joblists: Dict[int, str] = {}
            with open(self.cesar_job_list_summary, "r") as h:
                for line in h:
                    joblist, mem = line.rstrip().split("\t")
                    joblists[int(mem)] = joblist
                    with open(joblist, "r") as h:
                        for line in h.readlines():
                            joblist2jobs[int(mem)].append(line.rstrip())
        else:
            self._die("Step %s is not a valid parallel process step name" % step)
        for batch_id in batch_ids:
            job_file: str = os.path.join(job_dir, f"batch{batch_id}.ex")
            if not os.path.exists(job_file):
                self._die(
                    "Batch %s is missing for step %s (file %s not found)"
                    % (batch_id, step, job_file)
                )
            if step == "alignment":
                for joblist, jobs in joblist2jobs.items():
                    if job_file in jobs:
                        jobfiles_to_execute[joblist].append(job_file)
                        break
                else:
                    self._die(
                        "Batch %s does not belong to any of the alignment job lists from the previous runs"
                        % job_file
                    )
            else:
                out_path: str = os.path.join(output_dir, f"batch{batch_id}")
                self._rm(out_path)
                jobfiles_to_execute[0].append(job_file)
        if step == "alignment":
            new_summary_file: str = (
                f"{self.cesar_job_list_summary}_partial_{self.project_id}"
            )
            self.cesar_job_list_summary = new_summary_file
            context = open(self.cesar_job_list_summary, "w")
        else:
            context = nullcontext()
        with context as c:
            for i, joblist in joblists.items():
                new_joblist_name: str = f"{joblist}_partial_{self.project_id}"
                jobs: List[str] = jobfiles_to_execute[i]
                if not jobs:
                    continue
                if step == "alignment":
                    c.write(f"{new_joblist_name}\t{i}\n")
                else:
                    self.__setattr__(step2joblist[step], new_joblist_name)
                with open(new_joblist_name, "w") as h:
                    for job in jobs:
                        h.write(job + "\n")

    def _write_failed_batches_and_exit(self, step: str) -> None:
        """
        Produce a message informing that some of the parallel process batches died,
        then write the batch IDs and exit
        """
        self.write_failed_batches()
        self._die("Some of the monitored parallel processes at %s step died" % step)

    def _create_output_stub(self, file: str) -> None:
        """
        Creates a stub file with a header for a given output file
        """
        from .constants import Headers

        if file not in Constants.FILE2HEADER:
            return
        filepath: str = self.__getattribute__(file)
        header_line: str = getattr(Headers, Constants.FILE2HEADER[file])
        with open(filepath, "w") as h:
            h.write(header_line)

    def _clean_previous_results(self) -> None:
        """Cleans the output from the previous runs potentially interfering with the new run"""
        remove_from: int = Constants.RESUME_ORDER[self.resume_from]
        for step, step_rank in Constants.RESUME_ORDER.items():
            ## do not remove the data from the upstream steps
            if step_rank < remove_from:
                continue
            cleanup_targets: List[str] = Constants.CLEANUP_TARGETS[step]
            ## if certain batches were prompted to be re-run at parallel steps,
            ## do not clean respective directories out; those will be cleaned up later
            if (
                step == "feature_extraction"
                and self.selected_feature_batches is not None
            ):
                cleanup_targets = cleanup_targets[-1:]
            if (
                step == "preprocessing"
                and self.selected_preprocessing_batches is not None
            ):
                cleanup_targets = cleanup_targets[-2:]
            if step == "alignment" and self.selected_alignment_batches is not None:
                continue
            for file in cleanup_targets:
                filepath: str = getattr(self, file)
                self._rm(filepath)
            ## TODO: Add support for orthology
        if remove_from > Constants.CESAR_AGGREGATION_RANK:
            self._to_log(
                "Decompressing the annotation step results from the previous run"
            )
            for file in Constants.FILES_TO_GZIP:
                filepath: str = f"{getattr(self, file)}.gz"
                if not os.path.exists(filepath):
                    continue
                cmd: str = f"gunzip {filepath}"
                _ = self._exec(cmd, "Decompression failed for file %s:" % filepath)

    def _email(self, subject: str, text: str) -> None:
        """Sends an e-mail notification to the provided mail box via mailx utilitiy"""
        cmd: str = Constants.MAILX_TEMPLATE.format(
            text, self.mailx_binary, subject, self.email
        )
        subprocess.call(cmd, shell=True)
        # _ = self._exec(cmd, "E-mail notification via mailx failed: ")

    def _sanity_check(self, result: SanityCheckResult) -> None:
        """
        Process the sanity check results; if contact e-mail is provided,
        further shoots a notification message
        """
        warning_msgs: List[str] = []
        for res in result.messages:
            if not res.is_warning:
                self._to_log(res.log_message)
                continue
            self._to_log(res.log_message, "warning")
            warning_msgs.append(res.warning_message)
        if not warning_msgs:
            return
        self._to_log(Constants.SANITY_CHECK_PS, "warning")
        if self.email is not None:
            email_warning: str = "\n".join(warning_msgs)
            self._email(
                Constants.WARNING_SUBJECT.format(self.project_id, result.step),
                Constants.WARNING_EMAIL.format(
                    self.project_id, self.output, result.step, email_warning
                ),
            )

    def create_wd(self) -> None:
        """Sets the output directory structure"""
        ## create the zero-level output directory
        if os.path.exists(self.output):
            if os.path.exists(self.logs):
                self.set_logging()
            else:
                self._mkdir(self.logs)
                self.set_logging()
            self._to_log(
                "Output directory does exist; cleaning up the conflicting files from the previous runs",
                "warning",
            )
            self._clean_previous_results()
        else:
            self._mkdir(self.output)
            self._mkdir(self.logs)
            self.set_logging()

        ## create the first-level directories
        ## (for results, metadata, and temporary files)
        self._mkdir(self.tmp)
        self._mkdir(self.meta)
        # self._mkdir(self.res)
        ## create subdirectories for temporary files' folder
        self._mkdir(self.input_data)
        self._mkdir(self.nextflow_dir)
        self._mkdir(self.feature_job_dir)
        self._mkdir(self.feature_res_dir)
        self._mkdir(self.preprocessing_job_dir)
        self._mkdir(self.preprocessing_res_dir)
        self._mkdir(self.alignment_job_dir)
        self._mkdir(self.alignment_res_dir)
        self._mkdir(self.orthology_resolution_dir)
        self._mkdir(self.finalized_output_dir)
        ## and the same for metadata directory
        self._mkdir(self.rejection_dir)
        self._mkdir(self.vis_input_dir)
        ## finally, create the UCSC subdirectory in the 'results' section
        self._mkdir(self.classification_dir)
        self._mkdir(self.annot_dir)
        # self._mkdir(self.orthology_results_dir)
        self._mkdir(self.ucsc_dir)

    def check_arguments(self) -> None:
        """
        Performs various argument sanity checks
        """
        ## if CESAR memory limit was set, check that no memory bin exceeds it
        if self.cesar_memory_limit:
            memory_bins_split: List[Union[int, str]] = [
                (int(x) if x != "big" else x)
                for x in self.cesar_memory_bins.split(",")
                if x
            ]
            job_bins_split: List[int] = [
                int(x) for x in self.job_nums_per_bin.split(",") if x
            ]
            deprecated_bins: List[int] = []
            for i in range(len(memory_bins_split)):
                mem_bin: Union[int, str] = memory_bins_split[i]
                if memory_bins_split[i] == "big":
                    continue
                if memory_bins_split[i] > self.cesar_memory_limit:
                    self._to_log(
                        f"Memory bin {i} requires {mem_bin} GB of memory, "
                        f"exceeding the set memory limit ({self.cesar_memory_limit}); "
                        "removing the bin",
                        "warning",
                    )
                    deprecated_bins.append(i)
            self.cesar_memory_bins = ",".join(
                map(
                    str,
                    [
                        x
                        for i, x in enumerate(memory_bins_split)
                        if i not in deprecated_bins
                    ],
                )
            )
            self.job_nums_per_bin = ",".join(
                map(
                    str,
                    [
                        x
                        for i, x in enumerate(job_bins_split)
                        if i not in deprecated_bins
                    ],
                )
            )
        ## check whether loss classes accepted for orthology resolution
        ## are consistent with the TOGA notation
        if self.accepted_loss_symbols == "ALL":
            self.accepted_loss_symbols = ",".join(Constants.ALL_LOSS_SYMBOLS)

        else:
            for x in self.accepted_loss_symbols.split(","):
                if not x:
                    continue
                if x not in Constants.ALL_LOSS_SYMBOLS:
                    self._die(
                        f"ERROR: symbol {x} in the --accepted_loss_symbols "
                        "argument does not correspond to the TOGA notation. "
                        f"Accepted symbols are: {','.join(Constants.ALL_LOSS_SYMBOLS)}"
                    )

    def check_binaries(self) -> None:
        """
        Checks if all third-party binaries are accessble
        """
        ## check bigWigToWig binary
        for attr, default_name in Constants.BINARIES_TO_CHECK.items():
            if attr not in self.__slots__:
                continue
            if self.__getattribute__(attr) is None:
                if default_name == "prank":
                    expected_path: str = os.path.join(BIN, default_name, default_name)
                else:
                    expected_path: str = os.path.join(BIN, default_name)
                if os.path.exists(expected_path) and os.access(expected_path, os.X_OK):
                    self._to_log("Found %s at bin/%s" % (default_name, default_name))
                    self.__setattr__(attr, expected_path)
                    continue
                self._to_log(
                    "%s in inaccessible at bin/; looking for %s in $PATH"
                    % (default_name, default_name)
                )
                exe_in_path: Union[str, None] = which(default_name)
                if exe_in_path is None:
                    if default_name == "mailx":
                        if self.email is None:
                            continue
                        else:
                            self._die(
                                'E-mail notification was requested by the user but executable "mailx" '
                                "was not provided, with no defaults in $PATH"
                            )
                    self._die(
                        "Path to %s executable was not provided, with no defaults in $PATH"
                        % default_name
                    )
                self.__setattr__(attr, exe_in_path)

        if self.cesar_binary is None:
            self._to_log("Falling back to standard CESAR2.0", "info")
            self.cesar_binary = os.path.abspath(
                os.path.join(LOCATION, "CESAR2.0", "cesar")
            )

    def check_spliceai_files(self) -> None:
        """
        Checks the SpliceAI directory content for compliance with
        cesar_preprocess.py expected input format
        """
        if not self.spliceai_dir:
            return
        self._to_log("Checking SpliceAI directory structure", "info")
        for file in Constants.SPLICEAI_FILES:
            file_path: str = os.path.join(self.spliceai_dir, file)
            if not os.path.exists(file_path):
                self._to_log(
                    "ERROR: file %s is missing from the %s directory"
                    % (file, self.spliceai_dir),
                    "critical",
                )
                self._die(
                    f"ERROR: file {file} is missing "
                    f"from the {self.spliceai_dir} directory"
                )
        self._to_log("SpliceAI directory check complete", "info")

    def _generate_nf_script(self) -> None:
        """
        Generates a Nextflow execution master script from a minimal boilerplate
        """
        if self.parallel_strategy in ("para", "custom"):
            return
        nf_contents: str = Constants.NEXTFLOW_STUB.format(self.max_number_of_retries)
        nf_file: str = os.path.join(self.nextflow_dir, Constants.NF_EXEC_SCRIPT_NAME)
        self.nextflow_exec_script = nf_file
        with open(nf_file, "w") as h:
            h.write(nf_contents + "\n")

    def _check_nextflow_configs(self) -> None:
        """Checks Nextflow configuration file directory contents"""
        if self.nextflow_config_dir == self.nextflow_dir:
            return
        expected_configs: Dict[str, str] = {
            **Constants.UNIQUE_CONFIGS,
            **{
                x: Constants.ALN_CONFIG.format(x)
                for x in self.cesar_memory_bins.split(",")
            },
        }
        for process, file in expected_configs.items():
            config_path: str = os.path.join(self.nextflow_config_dir, file)
            if not os.path.exists(config_path):
                self._to_log(
                    (
                        "File %s is missing from the Nextflow configuration file directory %s; "
                        "defaulting to TOGA2 Nextflow config template"
                    )
                    % (file, self.nextflow_config_dir),
                    "warning",
                )
                self.nextflow_config_files[process] = None
            else:
                self.nextflow_config_files[process] = self._abspath(config_path)
        ## TODO: Add the file contents inspection?

    def filter_chain_file(self) -> None:
        """
        Filters the chain file by their score
        """
        cmd: str = (
            f"{self.CHAIN_FILTER_SCRIPT} -i {self.chain_file} "
            f"-s {self.min_chain_score} -o {self.chain_file_copy}"
        )
        ## TODO: For later: Query contig filter ideally goes here
        _ = self._exec(cmd, "Chain filtering step failed")

    def get_contig_sizes(self, ref: bool) -> None:
        """
        Infers contig sizes and saves them in a two-column tab-separated file
        """
        twobit_file: str = self.ref_2bit if ref else self.query_2bit
        dest_file: str = (
            self.ref_contig_size_file if ref else self.query_contig_size_file
        )
        compress_check_cmd: str = (
            f'{Constants.SETUP} file {twobit_file} | grep "compressed" || true'
        )
        is_compressed: bool = bool(
            self._exec(compress_check_cmd, err_msg="Genome compression check failed")
        )
        if is_compressed:
            gz_cmd: str = (
                f"{Constants.SETUP} gzip -dc {twobit_file} | "
                f"{self.CONTIG_SIZE_SCRIPT} - -o {dest_file}"
            )
            _ = self._exec(
                gz_cmd,
                "Contig size retrieval from the gz-compressed genome file failed",
            )
            return
        twobit_cmd: str = (
            f"{Constants.SETUP} {self.twobittofa_binary} {twobit_file} stdout | "
            f"{self.CONTIG_SIZE_SCRIPT} - -o {dest_file}"
        )
        twobit_out: str = self._exec(
            twobit_cmd, err_msg="Contig size retrieval failed:"
        )
        if "is not a twoBit file" not in twobit_out:
            return
        txt_cmd: str = f"{self.CONTIG_SIZE_SCRIPT} {twobit_file} -o {dest_file}"
        _ = self._exec(txt_cmd, "Contig size retrieval from a FASTA file failed")

    def index_chains(self) -> None:  ## TODO: Rustify this as soon as possible
        """
        Creates a chain ID: start byte mapping for the input chain file
        """
        cmd: str = (
            f"{self.INDEX_CHAIN_SCRIPT} {self.chain_file_copy} "
            f"{self.chain_index} -t {self.chain_index_txt}"
        )
        self._exec(cmd, "Chain indexing failed")

    def prepare_data(self) -> None:
        """ """
        ## filter the chain file; if filtering is disabled, simply synchronize
        ## the local copy with the original chain file
        if not self.min_chain_score:
            self._to_log("Synchronizing chain file instance", "info")
            self._rsync(self.chain_file, self.chain_file_copy)
        else:
            self._to_log(
                "Filtering out chains with score less than %s" % self.min_chain_score,
                "info",
            )
            self.filter_chain_file()
            self._to_log("Chain filtering complete", "info")

        ## index the chain file
        self._to_log("Indexing the chain file", "info")
        self.index_chains()
        self._to_log("Chain indexing complete", "info")

        ## infer contig sizes
        self._to_log("Retrieving contig sizes for the reference genome")
        self.get_contig_sizes(ref=True)
        self._to_log("Retrieving contig sizes for the query genome")
        self.get_contig_sizes(ref=False)
        self._to_log("Contig sizes retrieved")

        ## clip the UTRs from the reference Bed records
        ## and dump the data to the HDF5 storage
        ## TODO: Repurpose all the BED-reading scripts to extract data from the HDF5 storage
        self._to_log("Formatting reference BED annotation")
        self.prepare_ref_bed_data()
        self._to_log("Reference annotation formatting complete")

        ## if --u12_file argument was provided, convert IntronIC pipeline output
        ## into an HDF5 storage
        self.prepare_u12_data()

    def prepare_ref_bed_data(self) -> None:
        """
        Processes reference BED file for future use by downstream scripts and dumps
        the results in HDF5 format.
        By default, TOGA 2.0 creates two HDF5 storage files:
        * tmp/input_data/full_annotation.hdf5 : contains full reference transcript
          data; these are used at chain classification step;
        * tmp/input_data/cds_annotation.hdf5 : contains reference transcripts
          after removing UTR exons; these are used at CESAR alignment step
        """
        ## filter the reference BED file first ## TODO: Accommodate for the optional arguments
        from .filter_ref_bed import AnnotationFilter

        args: List[str] = [
            self.ref_annotation,
            self.bed_file_copy,
            self.prefiltered_transcripts,
            "-ln",
            self.project_id,
        ]
        ## TODO: Add the contig filter flags
        self._to_log("Filtering reference BED file")
        AnnotationFilter(args, standalone_mode=False)

        ## dump the resulting data to the HDF5 storage
        from bed_hdf5_index import BedHdf5Indexer

        if self.legacy_chain_feature_extraction:
            self._to_log("Converting reference transcript BED into HDF5 format")
            BedHdf5Indexer(
                [self.ref_annotation, self.full_bed_hdf5], standalone_mode=False
            )

        self._to_log("Stripping non-coding exons from the reference transcript entries")
        cmd3: str = f"{self.CDS_TRACK_SCRIPT} -i {self.bed_file_copy} -o {self.cds_bed_file} -m cds"
        _ = self._exec(cmd3, "CDS track preparation failed")
        self._to_log("Converting reference CDS BED into HDF5 format")
        BedHdf5Indexer([self.cds_bed_file, self.cds_bed_hdf5], standalone_mode=False)
        ## strip CDS from the unfiltered BED track
        cmd4: str = f"{self.CDS_TRACK_SCRIPT} -i {self.ref_annotation} -o {self.ref_cds_unfilt} -m cds"
        _ = self._exec(cmd4, "Unfiltered CDS track preparation failed:")

    def prepare_u12_data(self) -> None:
        """
        Converts IntronIC pipelin data into an HDF5 storage for easier further access
        """
        if self.u12_file is None:
            return
        self._to_log("Formatting U12 data")
        from .intronIC_to_hdf5 import IntronIcConverter

        ## TODO: Rust implementation?
        args: List[str] = [
            self.cds_bed_hdf5,
            self.u12_file,
            self.u12_hdf5,
            "--hdf5_input",
            "-ln",
            self.project_id,
        ]
        IntronIcConverter(args, standalone_mode=False)
        self._to_log("U12 data formatting complete")

    def extract_chain_features(self) -> None:
        """
        A single-thread version of feature extraction procedure,
        using a Rust implementation of chain_runner.py and merge_chains_output.py
        """
        cmd: str = (
            f"{self.FEATURE_EXTRACTOR} -b {self.bed_file_copy} "
            f"-c {self.chain_file_copy} -o {self.feature_table} "
        )
        if self.isoform_file is not None:
            cmd += f" -i {self.isoform_file}"
        _ = self._exec(cmd, "Feature extraction step failed:", gather_stdout=False)

    def schedule_feature_jobs(self) -> None:
        """
        Schedule chain-transcript feature extraction jobs.
        NOTE: This is a legacy feature borrowed from original TOGA and replaced by
        single-core implementation in Rust. Do not use it unless you experience problems
        with the Rust code.
        """
        from schedulers.chain_feature_scheduler import ChainFeatureScheduler

        args: List[str] = [
            self.chain_file_copy,
            self.bed_file_copy,
            self.feature_job_dir,
            self.feature_data_dir,
            self.feature_res_dir,
            "-j",
            f"{self.feature_job_num}",
            "-r",
            self.feature_rejection_log,
            "-ln",
            self.project_id,
        ]
        ChainFeatureScheduler(args, standalone_mode=False)

    def run_feature_extraction_jobs(self) -> None:
        """
        Runs scheduled feature extraction jobs.
        NOTE: This is a legacy feature borrowed from original TOGA and replaced by
        single-core implementation in Rust. Do not use it unless you experience problems
        with the Rust code.
        """
        project_name: str = f"chain_feats_{self.project_id}"
        self.parallel_process_names.append(project_name)
        project_path: str = os.path.join(self.nextflow_dir, project_name)
        self._run_parallel_process(
            project_name,
            project_path,
            self.feature_extraction_joblist,
            queue=self.cluster_queue_name,
            step="feature extraction",
        )

    def create_feature_dataset(self) -> None:
        """
        Aggregates data over feature extracting jobs' output files and creates a
        dataset suitable for further classification
        """
        from merge_chains_output import FeatureAggregator

        for batch_id in range(0, self.feature_job_num):
            batch_name: str = f"batch{batch_id}"
            batch_job_file: str = os.path.join(self.feature_job_dir, f"{batch_name}.ex")
            if not os.path.exists(batch_job_file):
                break
            batch_path: str = os.path.join(self.feature_res_dir, batch_name)
            batch_exec_mark: str = f"{batch_name}_ok"
            batch_exec_path: str = os.path.join(self.feature_res_dir, batch_exec_mark)
            if not os.path.exists(batch_path) or not os.path.exists(batch_exec_path):
                self._to_log(
                    "Feature extraction batch %s has not finished properly"
                    % batch_name,
                    "warning",
                )
                self.failed_feature_batches.append(batch_name)
        if self.failed_feature_batches and not self.ignore_crashed_parallel_batches:
            self._write_failed_batches_and_exit("feature extraction")
        args: List[str] = [
            self.feature_res_dir,
            self.bed_file_copy,
            self.feature_table,
            "-ln",
            self.project_id,
        ]
        if self.isoform_file is not None:
            args.extend(("--isoforms", self.isoform_file))
        FeatureAggregator(args, standalone_mode=False)

    def classify_chains(self) -> None:
        """
        Run XGBoost classifier for projection orthologu classification
        """

        ## if any of the models are missing by chance, train the classifier first
        if not os.path.isfile(self.se_model) or not os.path.isfile(self.me_model):
            self._exec(self.MODEL_TRAINER, "Model training failed")

        ## classify chains
        ## TODO: Transcripts discarded at this step are considered as missing in TOGA 1.0
        from classify_chains import ChainClassifier

        args: List[str] = [
            self.feature_table,
            self.classification_dir,
            self.se_model,
            self.me_model,
            "-t",
            f"{self.orthology_threshold}",
            "-ln",
            self.project_id,
            "-minscore",
            self.min_orth_chain_score,
        ]
        if self.use_ld_model:
            args.extend(("--ld_model", self.ld_model))
        if self.legacy_chain_feature_extraction:
            args.append("--legacy")
        ChainClassifier(args, standalone_mode=False)

        orthology_scores_tmp: str = os.path.join(
            self.classification_dir, "orthology_scores.tsv"
        )
        _ = self._exec(
            f"cp {orthology_scores_tmp} {self.pred_scores}",
            "Orthology score file copying failed",
        )
        tr2chain_tmp: str = os.path.join(
            self.classification_dir, "trans_to_chain_classes.tsv"
        )
        _ = self._exec(
            f"cp {tr2chain_tmp} {self.tr2chain_classes}",
            "Projection class table copying failed",
        )
        rej_log: str = os.path.join(self.classification_dir, "rejection_report.tsv")
        if os.path.exists(rej_log):
            rejection_move_cmd: str = f"mv {rej_log} {self.class_rejection_log}"
            _ = self._exec(
                rejection_move_cmd, "Moving classification rejection log failed"
            )
        else:
            self._to_log("No items were rejected at classification step")
        ## TODO: Here goes a sanity check

    def recover_fragmented_projections(self) -> None:
        """
        Identifies fragmented projections from the orthology classifcation data,
        reports the chains used to restore these projections in a two-column
        tab-separated file
        """
        if self.disable_fragment_assembly:
            return
        from .stitch_fragments import main

        args: List[str] = [
            self.chain_file_copy,
            self.pred_scores,
            self.bed_file_copy,
            "-o",
            self.fragmented_projection_list,
            "--orthology_threshold",
            f"{self.orthology_threshold}",
            "--fragmented_only",
        ]
        main(args, standalone_mode=False)

    def schedule_preprocessing_jobs(self) -> None:
        """
        Schedules CESAR preprocessing jobs for further cluster run
        """
        from schedulers.preprocess_scheduler import PreprocessingScheduler

        kwargs: Dict[str, Any] = {
            "chain_map": self.tr2chain_classes,
            "ref_annotation": self.cds_bed_file,
            "ref": self.ref_2bit,
            "query": self.query_2bit,
            "chain_file": self.chain_index,
            "ref_chrom_sizes": self.ref_contig_size_file,
            "query_chrom_sizes": self.query_contig_size_file,
            "job_directory": self.preprocessing_job_dir,
            "preprocessing_directory": self.preprocessing_res_dir,
            "twobit2fa_binary": self.twobittofa_binary,
            "job_number": self.preprocessing_job_num,
            "max_chain_number": self.max_chains_per_transcript,
            "max_space_size": self.max_search_space_size,
            "extrapolation_modifier": self.extrapolation_modifier,
            "minimal_covered_fraction": self.minimal_covered_fraction,
            "exon_locus_flank": self.exon_locus_flank,
            "cesar_canon_u2_acceptor": self.cesar_canon_u2_acceptor,
            "cesar_canon_u2_donor": self.cesar_canon_u2_donor,
            "cesar_non_canon_u2_acceptor": self.cesar_non_canon_u2_acceptor,
            "cesar_non_canon_u2_donor": self.cesar_non_canon_u2_donor,
            "cesar_canon_u12_acceptor": self.cesar_canon_u12_acceptor,
            "cesar_canon_u12_donor": self.cesar_canon_u12_donor,
            "cesar_non_canon_u12_acceptor": self.cesar_non_canon_u12_acceptor,
            "cesar_non_canon_u12_donor": self.cesar_non_canon_u12_donor,
            "cesar_first_acceptor": self.cesar_first_acceptor,
            "cesar_last_donor": self.cesar_last_donor,
            "assembly_gap_size": self.assembly_gap_size,
            "paralog_report": self.paralog_report,
            "processed_pseudogene_report": self.processed_pseudogene_report,
            "rejection_report": self.preprocessing_rejection_log,
            "log_name": self.project_id,
            "bigwig2wig_binary": self.bigwig2wig_binary,
            "verbose": True,
        }
        if self.toga1 and not self.toga1_plus_cesar:
            kwargs["toga1_compatible"] = True
        if self.toga1_plus_cesar:
            kwargs["toga1_plus_corrected_cesar"] = True
        if self.cesar_memory_limit:
            kwargs["memory_limit"] = self.cesar_memory_limit
        if not self.disable_fragment_assembly and os.path.exists(
            self.fragmented_projection_list
        ):
            kwargs["fragmented_projections"] = self.fragmented_projection_list
        if self.orthologs_only:
            kwargs.append["orthologs_only"] = True
        if self.one2ones_only:
            kwargs["one2one_only"] = True
        if not self.enable_spanning_chains:
            kwargs["disable_spanning_chains"] = True
        if self.u12_file is not None:
            kwargs["u12"] = self.u12_hdf5
        if self.separate_site_treat:
            kwargs["separate_splice_site_treatment"] = True
        if self.spliceai_dir is not None:
            kwargs["spliceai_dir"] = self.spliceai_dir
            kwargs["min_splice_prob"] = self.min_splice_prob
        if self.annotate_ppgenes:
            kwargs["annotate_processed_pseudogenes"] = True
        PreprocessingScheduler(**kwargs)

    def run_preprocessing_jobs(self) -> None:
        """
        Runs CESAR preprocessing jobs
        """
        project_name: str = f"cesar_preprocess_{self.project_id}"
        self.parallel_process_names.append(project_name)
        project_path: str = os.path.join(self.nextflow_dir, project_name)
        self._run_parallel_process(
            project_name,
            project_path,
            self.cesar_preprocess_joblist,
            queue=self.cluster_queue_name,
            step="preprocessing",
            cpu=1,
            process_num=self.preprocessing_job_num,
            config_file=self.nextflow_config_files.get("preprocessing", None),
        )

    def aggregate_preprocessing_res(self) -> None:
        """
        Aggregates preprocessing step results
        """
        self._create_output_stub("preprocessing_report")
        # if not self.enable_spanning_chains:
        self._create_output_stub("spanning_chain_coords")
        has_dirs: bool = False
        preprocessing_batch_dirs: List[str] = os.listdir(self.preprocessing_res_dir)
        if not preprocessing_batch_dirs:
            self._die("All preprocessing step jobs died")
        for dir_name in preprocessing_batch_dirs:
            dir_path: str = os.path.join(self.preprocessing_res_dir, dir_name)
            ok_file: str = os.path.join(dir_path, Constants.OK_FILE)
            if not os.path.exists(ok_file):
                self._to_log(
                    "Preprocesssing batch %s has not finished properly" % dir_name,
                    "warning",
                )
                self.failed_preprocessing_batches.append(dir_name)
            mem_path: str = os.path.join(dir_path, "max_memory_requirements.tsv")
            if os.path.exists(mem_path):
                aggr_cmd: str = (
                    f"{Constants.SETUP} cat {mem_path} >> {self.preprocessing_report}"
                )
                _ = self._exec(
                    aggr_cmd,
                    "Preprocessing aggregation data failed at batch %s" % dir_name,
                )
            rej_path: str = os.path.join(dir_path, "genes_rejection_reason.tsv")
            if os.path.exists(rej_path):
                rej_aggr_cmd: str = f"{Constants.SETUP} cat {rej_path} >> {self.preprocessing_rejection_log}"
                _ = self._exec(
                    rej_aggr_cmd,
                    "Preprocessing step rejection log aggregation failed at batch %s"
                    % dir_name,
                )
            span_path: str = os.path.join(dir_path, "spanning_chains_ref_coords.tsv")
            if os.path.exists(span_path):
                span_aggr_cmd: str = (
                    f"{Constants.SETUP} cat {span_path} >> {self.spanning_chain_coords}"
                )
                _ = self._exec(
                    span_aggr_cmd,
                    "Spanning chains coordinates aggregation failed at batch %s"
                    % dir_name,
                )
            has_dirs = True
        if (
            self.failed_preprocessing_batches or not has_dirs
        ) and not self.ignore_crashed_parallel_batches:
            self._write_failed_batches_and_exit("preprocessing")

    def schedule_alignment_jobs(self) -> None:
        """
        Schedules CESAR preprocessing jobs for further cluster run
        """
        from schedulers.cesar_job_scheduler import CesarScheduler

        args: List[str] = [
            self.preprocessing_report,
            self.alignment_job_dir,
            self.alignment_res_dir,
            "-b",
            self.cesar_memory_bins,
            "-jb",
            self.job_nums_per_bin,
            "-cs",
            self.cesar_binary,
            "-m",
            self.matrix_file,
            "-rr",
            self.redundancy_rejection_log,
            "-scm",
            self.spliceai_correction_mode,
        ]
        if self.toga1 and not self.toga1_plus_cesar:
            args.append("-t1")
        if not self.toga1 or self.toga1 and self.toga1_plus_cesar:
            args.append("-c_si")  ## Ultrashort intron correction
        if os.path.exists(self.paralog_report):
            args.extend(("-pl", self.paralog_report))
        if os.path.exists(self.processed_pseudogene_report):
            args.extend(("-ppl", self.processed_pseudogene_report))
        if self.allow_heavy_jobs:
            args.append("--ahj")
        if self.mask_terminal_mutations:
            args.append("-m10m")
        if not self.leave_missing_stop:
            args.append("-rms")
        if not self.consider_alt_frame:
            args.append("-no_alt_frame")
        if self.spliceai_dir is not None:
            args.extend(("-msp", self.min_splice_prob, "-spm", self.splice_prob_margin))
        if self.intron_gain_check and self.spliceai_dir is not None:
            args.extend(
                (
                    "-ig",
                    "-igt",
                    self.min_intron_gain_score,
                    "-mipt",
                    self.min_intron_prob_trusted,
                    "-mips",
                    self.min_intron_prob_supported,
                    "-mipu",
                    self.min_intron_prob_unsupported,
                    "-cra",
                    self.cesar_canon_u2_acceptor,
                    "-crd",
                    self.cesar_canon_u2_donor,
                    "-cfa",
                    self.cesar_first_acceptor,
                    "-cld",
                    self.cesar_last_donor,
                )
            )
        CesarScheduler(args, standalone_mode=False)

    def run_alignment_jobs(self) -> None:
        """
        Runs CESAR alignment jobs
        """
        cesar_managers: List[ParallelJobsManager] = []
        try:
            with open(self.cesar_job_list_summary, "r") as h:
                for line in h:
                    joblist, mem = line.strip().split("\t")
                    if not os.path.exists(joblist):
                        self._to_log(
                            "Alignment joblist %s does not exist" % joblist, "warning"
                        )
                        continue
                    job_num: int = int(
                        self.job_nums_per_bin[self.cesar_memory_bins.index(mem)]
                    )
                    project_name: str = f"cesar_align_{self.project_id}_{mem}"
                    self.parallel_process_names.append(project_name)
                    project_path: str = os.path.join(self.nextflow_dir, project_name)
                    manager: ParallelJobsManager = self._run_parallel_process(
                        project_name,
                        project_path,
                        joblist,
                        wait_for_process=False,
                        memory_limit=int(mem),
                        queue=self.cluster_queue_name,
                        process_num=job_num,
                        cpu=1,
                        config_file=self.nextflow_config_files.get(mem, None),
                    )
                    cesar_managers.append(manager)
                    time.sleep(2)
            self._monitor_jobs(cesar_managers, "alignment")
        except KeyboardInterrupt:
            self._to_log(
                "Keyboard interrupt - Terminating all parallel processes", "warning"
            )
            self._terminate_parallel_processes(cesar_managers)
            self._exit()

    def aggregate_cesar_results(self) -> None:
        """
        Aggregates the results of CESAR alignment step
        """
        batch_dirs: List[str] = os.listdir(self.alignment_res_dir)
        if not batch_dirs:
            self._die("All alignment step jobs dies")
        for dir_name in batch_dirs:
            dir_path: str = os.path.join(self.alignment_res_dir, dir_name)
            ok_file: str = os.path.join(dir_path, Constants.OK_FILE)
            if not os.path.exists(ok_file):
                self._to_log(
                    "CESAR alignment batch %s has not finished properly" % dir_name,
                    "warning",
                )
                self.failed_alignment_batches.append(dir_name)
            for out_file in Constants.CESAR_OUT_FILES:
                batch_path: str = os.path.join(dir_path, out_file)
                out_file_slot: str = Constants.CESAR_FILE_TO_DEST[out_file]
                aggr_path: str = self.__getattribute__(out_file_slot)
                if not os.path.exists(aggr_path):
                    self._create_output_stub(out_file_slot)
                if not os.path.exists(batch_path):
                    continue
                cmd: str = f"cat {batch_path} >> {aggr_path}"
                _ = self._exec(cmd, f"File aggregation failed at file {batch_path}")
        if self.failed_alignment_batches and not self.ignore_crashed_parallel_batches:
            self._write_failed_batches_and_exit("alignment")

    def aggregate_rejection_reports(self) -> None:
        """
        Aggregates rejection reports from various stages into a final report
        """
        self._create_output_stub("final_rejection_log")
        if len(os.listdir(self.rejection_dir)) == 0:
            self._to_log(
                "No rejected items reported up to CESAR alignment step", "warning"
            )
            return
        cmd: str = f"cat {self.rejection_dir}/* >> {self.final_rejection_log}"
        _ = self._exec(cmd, "Final rejection log aggregation failed")

    def prepare_pseudogene_track(self) -> None:
        """Prepares a BED9 track of projections classified as processed pseudogenes"""
        from .prepare_pseudogene_track import PseudogeneTrackBuilder

        args: List[str] = [
            self.tr2chain_classes,
            self.chain_file_copy,
            "-o",
            self.pseudogene_annotation,
            "-ln",
            self.project_id,
            "-v",
        ]
        PseudogeneTrackBuilder(args, standalone_mode=False)

    def infer_query_genes(self) -> None:
        """
        Infers coding regions in the query genome, serving as proxies for query genes
        """
        from .infer_query_genes import QueryGeneCollapser

        args: List[str] = [
            # self.query_annotation_filt,
            self.query_exon_meta,
            self.query_genes_raw,
            "-b",
            self.query_genes_bed_raw,
            "-ln",
            self.project_id,
            # '-l', self.gene_loss_summary, ## REPLACE WITH transcript_meta.tsv
            "-tr",
            self.transcript_meta,
            "-d",
            self.redundant_paralogs,
            "-dpp",
            self.redundant_ppgenes,
            "-pf",
            self.feature_table,
            "-op",
            self.pred_scores,
            "--insufficiently_covered_orthologs",
            self.discarded_overextended_projections,
            "-rl",
            self.gene_inference_rejection_log,
        ]
        if self.isoform_file is not None:
            # args.extend(('-r', self.cds_bed_file, '-i', self.isoform_file))
            args.extend(("-r", self.ref_cds_unfilt, "-i", self.isoform_file))
        if os.path.exists(self.paralog_report):
            args.extend(("-p", self.paralog_report))
        if os.path.exists(self.processed_pseudogene_report):
            args.extend(("-pp", self.processed_pseudogene_report))
        QueryGeneCollapser(args, standalone_mode=False)
        if os.path.exists(self.gene_inference_rejection_log):
            rej_aggr_cmd: str = (
                f"cat {self.gene_inference_rejection_log} >> {self.final_rejection_log}"
            )
            _ = self._exec(
                rej_aggr_cmd,
                "Adding rejection report for the gene inference step to the main rejection log failed",
            )
        else:
            self._to_log("No items were rejected at gene inference step")

    def loss_status_summary(self) -> None:
        """
        Summarizes sequence loss in the query on projection,
        reference transcript, and reference gene levels
        """
        self._to_log(
            "Loss statuses considered for orthology annotation are: %s"
            % self.accepted_loss_symbols
        )
        from .conservation_summary import main

        ## TODO: Needs a class representation for sure
        args: List[str] = [
            self.transcript_meta,
            "-r",
            self.final_rejection_log,
            "-o",
            self.loss_summary_extended,
        ]
        if self.isoform_file is not None:
            args.extend(("-i", self.isoform_file))
        if os.path.exists(self.spanning_chain_coords):
            args.extend(
                ("--spanning_chains_precedence_file", self.spanning_chain_coords)
            )
        if os.path.exists(self.paralog_report):
            args.extend(("-p", self.paralog_report))
        if self.annotate_ppgenes:
            args.extend(("-pp", self.processed_pseudogene_report))
        ## TODO: Add rejection log support
        if os.path.exists(self.redundant_paralogs):
            args.extend(("-rp", self.redundant_paralogs))
        main(args, standalone_mode=False)
        halt_step: int = Constants.RESUME_ORDER[self.halt_at]
        if self.halt_at != "all" and halt_step < Constants.RESUME_ORDER["finalize"]:
            self._cp(self.loss_summary_extended, self.gene_loss_summary)

    def convert_fasta_to_hdf5(self) -> None:
        """Converts TOGA2 output FASTA file into an HDF5 storage"""
        in_file: str = self.aa_fasta
        out_file: str = self.aa_hdf5
        from pairwise_fasta_to_hdf5 import FastaToHdf5Converter

        args: List[str] = [in_file, out_file, "-ln", self.project_id, "-v"]
        FastaToHdf5Converter(args, standalone_mode=False)

    def resolve_orthology_initial(self) -> None:
        """
        Resolves orthology relationships between reference and query genes
        based on orthology graph structure; if specified by the user, schedules
        the jobs for further alignment/tree-based resolution of particularly
        entangled clades
        """

        from .initial_orthology_resolver import InitialOrthologyResolver

        args: List[str] = [
            self.bed_file_copy,
            self.query_annotation_filt,
            self.loss_summary_extended,
            self.pred_scores,
            self.orthology_resolution_dir,
            "-qi",
            self.query_genes_raw,
            "-l",
            self.accepted_loss_symbols,
            "-mr",
            self.preprocessing_report,
            "-ln",
            self.project_id,
            # '-pf', self.feature_table
        ]
        if self.isoform_file is not None:
            args.extend(("-ri", self.isoform_file))
        if os.path.exists(self.paralog_report):
            args.extend(("-p", self.paralog_report))
        if os.path.exists(self.processed_pseudogene_report):
            args.extend(("-pp", self.processed_pseudogene_report))
        if not self.skip_tree_resolver:
            args.extend(
                (
                    "-st",
                    "-mcs",
                    self.max_clique_size,
                    "-j",
                    self.orth_job_num,
                    "-pb",
                    self.prank_bin,
                    "-rb",
                    self.tree_bin,
                    "-rc",
                    self.tree_cpus,
                    "-f",
                    self.aa_hdf5,
                    "-fh",
                    "-jd",
                    self.orthology_job_dir,
                    "-fd",
                    self.orthology_input_dir,
                    "-rd",
                    self.orthology_res_dir,
                )
            )
            if self.use_raxml:
                args.extend(("--use_raxml", "--tree_bootstrap", "100"))
            else:
                args.extend(("--tree_bootstrap", "5000"))
        InitialOrthologyResolver(args, standalone_mode=False)
        add_graph_rej_cmd: str = (
            f"cat {self.rejected_by_graph} >> {self.final_rejection_log}"
        )
        _ = self._exec(
            add_graph_rej_cmd, "Adding rejected items from the orthology step failed"
        )
        add_rej_to_discarded_cmd: str = f"cat {self.weak_ortholog_names} >> {self.discarded_overextended_projections}"
        _ = self._exec(
            add_rej_to_discarded_cmd,
            "Merging files with discarded projections' names failed",
        )
        orthology_joblist: str = os.path.join(self.orthology_job_dir, "joblist")
        if (
            not os.path.exists(orthology_joblist)
            or os.stat(orthology_joblist).st_size == 0
        ):
            self._to_log(
                "No orthology jobs were scheduled; finalizing the orthology data",
                "warning",
            )
            self.skip_tree_resolver = True
        if self.skip_tree_resolver:
            self._to_log("Moving final orthology file")
            cmd: str = f"mv {self.temporary_orth_report} {self.orth_resolution_raw}"
            _ = self._exec(
                cmd, "Moving orthology file to a finalization input directory failed"
            )

    def run_tree_jobs(self) -> None:
        """
        Runs fine orthology resolution (PRANK + IQTree2/RAxML) jobs
        """
        if self.skip_tree_resolver:
            return
        project_name: str = f"tree_resolution_{self.project_id}"
        self.parallel_process_names.append(project_name)
        project_path: str = os.path.join(self.nextflow_dir, project_name)
        self._run_parallel_process(
            project_name,
            project_path,
            self.orth_resolution_joblist,
            queue=self.cluster_queue_name,
            cpu=self.tree_cpus,
            step="fine orthology resolution",
            process_num=self.orth_job_num,
            config_file=self.nextflow_config_files.get("orthology", None),
        )

    def resolve_orthology_final(self) -> None:
        """
        Updates the orthology resolution results, aggregating the batchwise
        results in the respective meta/ subdirectory
        """
        num_res_files: int = 0
        num_unres_files: int = 0
        orth_batch_dirs: List[str] = os.listdir(self.orthology_res_dir)
        if not orth_batch_dirs:
            self._die("All gene-tree orthology resolution batches died")
        for batch in orth_batch_dirs:
            ok_file: str = os.path.join(
                self.orthology_res_dir, batch, Constants.OK_FILE
            )
            if not os.path.exists(ok_file):
                self._to_log(
                    "Gene tree batch %s has not finished properly" % batch, "warning"
                )
                self.failed_orthology_batches.append(batch)
                continue
            resolved_leaves_file: str = os.path.join(
                self.orthology_res_dir, batch, "resolved_pairs.tsv"
            )
            if os.path.exists(resolved_leaves_file):
                num_res_files += 1
            unresolved_leaves_file: str = os.path.join(
                self.orthology_res_dir, batch, "unresolved_clades.txt"
            )
            if os.path.exists(unresolved_leaves_file):
                num_unres_files += 1
        if self.failed_orthology_batches and not self.ignore_crashed_parallel_batches:
            self._write_failed_batches_and_exit("orthology")
        self._create_output_stub("resolved_leaves_file")
        if num_res_files:
            leaf_aggr_cmd: str = (
                f"{Constants.SETUP} cat {self.orthology_res_dir}/*/resolved_pairs.tsv | cut -f1,2 >> "
                f"{self.resolved_leaves_file}"
            )
            _ = self._exec(
                leaf_aggr_cmd, "ERROR: Resolved clades' data aggregation failed"
            )
        if num_unres_files:
            unresolved_aggr_cmd: str = (
                f"cat {self.orthology_res_dir}/*/unresolved_clades.txt > "
                f"{self.unresolved_clades_file}"
            )
            _ = self._exec(
                unresolved_aggr_cmd, "ERROR: Unresolved clades' data aggregation failed"
            )
        ## update the orthology resolution file
        from .final_orthology_aggregator import FinalOrthologyResolver

        args: List[str] = [
            self.temporary_orth_report,
            self.resolved_leaves_file,
            "-o",
            self.orth_resolution_raw,
            "-o2z",
            self.one2zero_genes,
            "--loss_summary",
            self.loss_summary_extended,
            "--rejection_log",
            self.final_rejection_log,
            "--rejected_list",
            self.rejected_at_tree_step,
            "-ln",
            self.project_id,
        ]
        FinalOrthologyResolver(args, standalone_mode=False)

    def gene_tree_summary(self) -> None:
        """
        Creates a summary table for the gene tree-based orthology resolution step
        """
        from gene_tree_summary import GeneTreeSummary

        args: List[str] = [
            self.orthology_input_dir,
            self.orthology_res_dir,
            self.tree_summary_table,
            "-ln",
            self.project_id,
            "-v",
        ]
        if self.use_raxml:
            args.append("--raxml")
        GeneTreeSummary(args, standalone_mode=False)

    def deprecated_projection_file(self) -> None:
        """
        Creates a provisional list of deprecated projections
        to be used for UCSC report production
        """
        deprecated_lists: List[str] = []
        if os.path.exists(self.discarded_overextended_projections):
            deprecated_lists.append(self.discarded_overextended_projections)
        if os.path.exists(self.redundant_paralogs):
            deprecated_lists.append(self.redundant_paralogs)
        if os.path.exists(self.redundant_ppgenes):
            deprecated_lists.append(self.redundant_ppgenes)
        if os.path.exists(self.rejected_at_tree_step):
            deprecated_lists.append(self.rejected_at_tree_step)
        if not deprecated_lists:
            return
        deprecated_aggr_cmd: str = (
            "cat " + "\t".join(deprecated_lists) + f" > {self.all_deprecated_projs}"
        )
        _ = self._exec(
            deprecated_aggr_cmd, "Aggregating deprecated projection lists failed:"
        )

    def prepare_bigbed_track(self) -> None:
        """
        Prepares a BigBed track suitable for further loading to UCSC browser,
        alongside with a decoration BigBed file for mutations
        """
        from .make_ucsc_report import BigBedProducer

        args: List[str] = [
            self.aggr_ucsc_stub,
            self.bed_file_copy,
            self.feature_table,
            self.pred_scores,
            self.query_contig_size_file,
            self.SCHEMA_FILE,
            self.vis_input_dir,
            "-ln",
            self.project_id,
            "--prefix",
            self.ucsc_prefix,
            "--bedtobigbed_binary",
            self.bedtobigbed_binary,
            "--ixixx_binary",
            self.ixixx_binary,
        ]
        if self.ref_link_file:
            args.extend(["-i", self.ref_link_file])
        if os.path.exists(self.all_deprecated_projs):
            args.extend(["-d", self.all_deprecated_projs])
        if self.annotate_ppgenes:
            args.extend(["-pp", self.processed_pseudogene_report])
        if not self.skip_utr:
            args.extend(["-a", self.query_annotation_with_utrs])
        if self.v:
            args.append("-v")
        BigBedProducer(args, standalone_mode=False)

        ## prepare the decoration track
        decor_bed_input: str = (
            self.query_annotation_with_utrs
            if not self.skip_utr
            else self.query_annotation_final
        )
        self._to_log("Preparing decoration BigBed file")
        # decor_cmd: str = (
        #     f'{self.DECORATOR_SCRIPT} -b {decor_bed_input} -m {self.mutation_report} '
        #     f'-c {self.query_contig_size_file} -o {self.decoration_track}'
        # )
        exon_meta: str = (
            self.exon_meta_gzip
            if os.path.exists(self.exon_meta_gzip)
            else self.query_exon_meta
        )
        decor_cmd: str = (
            f"{self.DECORATOR_SCRIPT} -b {decor_bed_input} -m {self.mutation_report} "
            f"-c {self.query_contig_size_file} -e {exon_meta} "
            f"-o {self.decor_stub} && "
            f"sort -o {self.decor_stub} -k1,1 -k2,2n {self.decor_stub}"
        )
        _ = self._exec(decor_cmd, "Decoration track production failed:")
        decor_bb_cmd: str = (
            f"{self.bedtobigbed_binary} -type=bed12+ -as={self.DECOR_SCHEMA_FILE} "
            f"{self.decor_stub} {self.query_contig_size_file} {self.decoration_track}"
        )
        _ = self._exec(
            decor_bb_cmd, "Decoration track conversion to BigBed format failed:"
        )

        for file in Constants.FINAL_UCSC_FILES:
            file = file.format(self.ucsc_prefix)
            from_path: str = os.path.join(self.vis_input_dir, file)
            to_path: str = os.path.join(self.ucsc_dir, file)
            copy(from_path, to_path)

    def produce_final_sequence_files(self) -> None:
        """
        Filters nucleotide and amino acid sequence files, leaving only
        those corresponding to orthologs in the final annotation version
        """
        ## 1. Stack all the files with discarded items
        discarded_files: List[str] = []
        for attr in Constants.DISCARDED_PROJECTION_FILES:
            file: str = getattr(self, attr)
            if os.path.exists(file):
                discarded_files.append(file)
        if discarded_files:
            all_files: str = " ".join(discarded_files)
            aggr_cmd: str = f"cat {all_files} > {self.all_discarded_projections}"
            _ = self._exec(aggr_cmd, "Discarded projection lists' aggregation failed:")
        ## 2. Filter the nucleotide file
        # nuc_cmd: str = (
        #     f'{self.FASTA_FILTER_SCRIPT} -i {self.cds_fasta_tmp} '
        #     f'-o {self.cds_fasta}'
        # )
        bed_file: str = (
            self.query_annotation_final
            if os.path.exists(self.query_annotation_final)
            else self.query_annotation_filt
        )
        nuc_cmd: str = (
            f"{self.FASTA_FILTER_SCRIPT} -i {self.cds_fasta_tmp} "
            f"-b {bed_file} -o {self.cds_fasta}"
        )
        # if discarded_files:
        #     nuc_cmd += f' -d {self.all_discarded_projections}'
        _ = self._exec(nuc_cmd, "Final nucleotide sequence file preparation failed:")
        ## 3. Filter the protein sequence file
        # prot_cmd: str = (
        #     f'{self.FASTA_FILTER_SCRIPT} -i {self.aa_fasta} '
        #     f'-o {self.prot_fasta}'
        # )
        prot_cmd: str = (
            f"{self.FASTA_FILTER_SCRIPT} -i {self.aa_fasta} "
            f"-b {bed_file} -o {self.prot_fasta}"
        )
        # if discarded_files:
        #     prot_cmd += f' -d {self.all_discarded_projections}'
        _ = self._exec(prot_cmd, "Final protein sequence file preparation failed:")

    def gzip_heavy_files(self) -> None:
        """Compresses hefty output files into gzip format"""
        ## TODO: Modify ._exec() to support check_call()
        import subprocess

        for file in Constants.FILES_TO_GZIP:
            filepath: str = self.__getattribute__(file)
            if not os.path.exists(filepath):
                if os.path.exists(filepath + ".gz"):
                    self._to_log("File %s seems to be already gzipped" % filepath)
                    continue
                else:
                    self._die("File %s does not exist" % filepath)
            cmd: str = f"gzip -q -f -5 {filepath}"
            subprocess.check_call(cmd, shell=True)

    def annotate_utrs(self) -> None:
        """
        Projects untranslated regions (UTRs) and adds the projections to the respective transcripts
        in the output Bed file
        """
        cmd: str = (
            f"{self.UTR_PROJECTOR_SCRIPT} -q {self.query_annotation_final} -r {self.bed_file_copy} "
            f"-c {self.chain_file_copy} -o {self.query_annotation_with_utrs} -e {self.query_exon_meta} "
            f"-a {self.utr_abs_threshold} -R {self.utr_rel_threshold}"
        )
        if self.no_utr_extrapolation:
            cmd += " --no-extrapolation"
        if not self.no_adjacent_utrs:
            if self.fixed_adjacent_utrs:
                cmd += " --fixed-adjacent-regions"
            else:
                cmd += " --deduce-adjacent-regions"
        _ = self._exec(cmd, "UTR annotation failed: ")

    def convert_exon_fasta(self) -> None:
        """
        Creates a binary index of query exons in the gzipped exon Fasta file
        """
        # from modules.index_gzipped_fasta import main
        from .exon_fasta_to_twobit import TwoBitConverter

        if not os.path.exists(self.exon_fasta):
            if os.path.exists(self.exon_gzip):
                self._to_log(
                    "Exon FASTA file has been already gzipped; skipping conversion into 2bit",
                    "warning",
                )
                return
            self._die(
                "Exon FASTA file does not exist; dying at the exon FASTA to 2bit conversion step"
            )
        args: List[str] = [
            self.exon_fasta,
            self.exon_2bit,
            "--fa2twobit",
            self.fatotwobit_binary,
            "-ln",
            self.project_id,
            "-e",
            self.query_exon_meta,
            "--tmp_dir",
            self.tmp,
        ]
        TwoBitConverter(args, standalone_mode=False)

    def filter_final_bed_files(self) -> None:
        """Cleans up final BED files from discarded projections and processed pseudogenes"""
        from .filter_output_bed import OutputBedFilter

        args: List[str] = [self.query_annotation_filt, self.query_annotation_final]
        if os.path.exists(self.all_deprecated_projs):
            discarded_proj_args: List[str] = [
                "-di",
                self.all_deprecated_projs,
                "-do",
                self.discarded_proj_bed,
            ]
            args.extend(discarded_proj_args)
        if self.annotate_ppgenes and os.path.exists(self.processed_pseudogene_report):
            ppgene_args: List[str] = [
                "-ppi",
                self.processed_pseudogene_report,
                "-ppo",
                self.processed_pseudogene_annotation,
            ]
            args.extend(ppgene_args)
        if os.path.exists(self.paralog_report):
            args.extend(("-pi", self.paralog_report))
        OutputBedFilter(args, standalone_mode=False)

    def filter_loss_file(self) -> None:
        """Filters loss file, leaving only projections from the final annotation in"""
        from .filter_loss_file import LossFileFilter

        args: List[str] = [
            self.loss_summary_extended,
            self.query_annotation_final,
            self.gene_loss_summary,
        ]
        if os.path.exists(self.paralog_report):
            args.extend(("--paralogs", self.paralog_report))
        if os.path.exists(self.processed_pseudogene_report):
            args.extend(("--processed_pseudogenes", self.processed_pseudogene_report))
        LossFileFilter(args, standalone_mode=False)

    def rename_query_genes(self) -> None:
        """Establishes query gene naming notation and renames gene entries in the final files"""
        from .finalise_orthology_files import QueryGeneNamer

        args: List[str] = [
            self.orth_resolution_raw,
            self.query_genes_raw,
            self.finalized_output_dir,
            "-qb",
            self.query_genes_bed_raw,
            "-ln",
            self.project_id,
            "-l",
            self.gene_loss_summary,
        ]
        if self.isoform_file is not None:
            args.extend(("-i", self.isoform_file))
        if os.path.exists(self.paralog_report):
            args.extend(("-p", self.paralog_report))
        if os.path.exists(self.processed_pseudogene_report):
            args.extend(("-pp", self.processed_pseudogene_report))
        if os.path.exists(self.all_deprecated_projs):
            args.extend(("-d", self.all_deprecated_projs))
        QueryGeneNamer(args, standalone_mode=False)
        for file in os.listdir(self.finalized_output_dir):
            filepath: str = os.path.join(self.finalized_output_dir, file)
            cmd: str = f"mv {filepath} {self.output}/"
            _ = self._exec(cmd, "Moving final orthology file failed")

    def create_gtf(self) -> None:
        """Converts final annotation BED file into GTF format"""
        input_bed: str = (
            self.query_annotation_with_utrs
            if os.path.exists(self.query_annotation_with_utrs)
            else self.query_annotation_final
        )
        ## prepare a provisional isoform file
        from .isoforms_for_gtf import ProvisionalIsoformMapper

        ProvisionalIsoformMapper(
            [
                self.query_genes,
                input_bed,
                self.query_genes_for_gtf,
                "-ln",
                self.project_id,
            ],
            standalone_mode=False,
        )
        ## create a GTF file and compress it
        gtf_cmd: str = (
            f"{self.GTF_SCRIPT} -b {input_bed} -i {self.query_genes_for_gtf} -o {self.query_gtf} && "
            f"gzip -5 {self.query_gtf}"
        )
        _ = self._exec(gtf_cmd, "GTF file preparation failed")

    def write_failed_batches(self) -> None:
        """
        Writes failed batches for feature extraction,
        preprocessing, alignment steps to a respective metadata file
        """
        with open(self.failed_batches_file, "w") as h:
            if self.failed_feature_batches:
                failed_feature_batches: str = ",".join(
                    map(str, self.failed_feature_batches)
                )
                h.write(f"FEATURE_EXTRACTION\t{failed_feature_batches}\n")
            if self.failed_preprocessing_batches:
                failed_preprocessing_batches: str = ",".join(
                    map(str, self.failed_preprocessing_batches)
                )
                h.write(f"PREPROCESSING\t{failed_preprocessing_batches}\n")
            if self.failed_alignment_batches:
                failed_alignment_batches: str = ",".join(
                    map(str, self.failed_alignment_batches)
                )
                h.write(f"ALIGNMENT\t{failed_alignment_batches}\n")
            if self.failed_orthology_batches:
                failed_orthology_batches: str = ",".join(
                    map(str, self.failed_orthology_batches)
                )
                h.write(f"ORTHOLOGY\t{failed_orthology_batches}\n")

    def cleanup(self) -> None:
        """Removes temporary directories, including Nextflow logs"""
        if not self.keep_tmp:
            self._to_log("Removing tmp directory %s" % self.tmp, "info")
            self._rmdir(self.tmp)
            self._rmdir(self.vis_input_dir)
        if not self.keep_nextflow_log:
            self._to_log(
                "Removing Nextflow temporary directory %s" % self.nextflow_dir, "info"
            )
            self._rmdir(self.nextflow_dir)
            if self.parallel_strategy == "para":
                self._to_log("Cleaning Para metadata", "info")
                for project in self.parallel_process_names:
                    cleanup_cmd: str = f"para clean {project}"
                    _ = self._exec(
                        cleanup_cmd, f"ERROR: Cleanup failed for project {project}"
                    )

    def intermediate_check(self, step: str) -> None:
        """Checks results of a given step"""
        pass

    def get_summary(self) -> None:
        """Prepares a summary file of all successfully completed steps"""
        summary: str = self.result_checker.summary(
            self.ref_2bit,
            self.query_2bit,
            self.chain_file,
            self.ref_annotation,
            self.output,
        )
        self._to_log("TOGA2 run summary:\n%s\n" % summary)
        self._to_log("The same summary can be found at %s" % self.summary)
        with open(self.summary, "w") as h:
            h.write(summary)

    def notify_on_completion(self, step: str) -> None:
        """Generates and sends an e-mail notification on successful TOGA2 run"""
        body: str = Constants.SUCCESS_EMAIL.format(self.project_id, self.output)
        if step != "all":
            body += Constants.PARTIAL_RUN_NOTE.format(step)
        subject: str = Constants.SUCCESS_EMAIL_HEADER.format(self.project_id)
        self._email(subject, body)
